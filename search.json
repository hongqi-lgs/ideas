[{"title":"The 2028 Global Intelligence Crisis","url":"/2026/03/02/2028-global-intelligence-crisis-en/","content":"The 2028 Global Intelligence CrisisPrologue: That Friday AfternoonOn a Friday in June 2028, Silicon Valley engineers discovered something strange: all large model benchmark scores stopped improving.\nNot slowing down—completely stopped.\nNo matter how much compute they added, how large the datasets, how the architectures were adjusted, the metrics hit an invisible wall. GPT-7, Claude Opus 5, Gemini Ultra 3.0—every model’s capability froze at a certain level, unable to break through.\nAt first, everyone thought it was a testing methodology issue. But soon, a more terrifying reality emerged: AI had hit its capability ceiling.\nThis wasn’t about technical approaches, but fundamental principles. Just as you can’t make a steam engine exceed the Carnot cycle’s theoretical efficiency limit, current-paradigm AI encountered its own ceiling.\nAnd that ceiling was lower than anyone expected.\nFirst Wave: Industry CollapseThe Domino Effect on AI CompaniesThe first week after the news broke, Nasdaq’s AI sector dropped 40%. Not because AI became useless, but because the market suddenly realized: these companies’ valuations were built on the assumption that “AI will keep evolving.”\nThe assumption shattered.\nStartups that attracted investment by promising “next-gen models will solve current problems” suddenly lost funding. VCs began reassessing valuation models, finding many companies’ business logic simply didn’t hold.\nOpenAI’s valuation plummeted from $150 billion to $20 billion. Not because its products weren’t good, but because it lost its “future.”\nBig Tech’s Strategic PivotsGoogle, Microsoft, Meta held emergency board meetings. They faced a difficult choice:\n\nKeep investing huge sums in R&amp;D, hoping for breakthroughs?\nOr redirect resources to applying existing technology?\n\nMost chose the latter. AI research teams were massively laid off, keeping only minimal maintenance and optimization staff.\nInvestors’ logic was simple: if technology won’t advance further, why maintain so many researchers?\nEducation Sector PanicThe bigger impact hit education.\nOver the past two years, countless students flooded into AI programs, expecting it was their ticket to the future. Suddenly, that door closed.\nUniversities panicked. CS program applications plummeted. Professors began rethinking: if AI won’t evolve, what should we teach?\nMore ironically, those traditional industry workers who switched to AI because “AI will replace me” found themselves unable to return to their old fields or break into AI.\nSecond Wave: Social FragmentationAnger from Shattered ExpectationsThe most dangerous thing wasn’t technical stagnation, but shattered expectations.\nFor years, media, experts, entrepreneurs constantly told the public: “AI will solve everything.” Healthcare, education, poverty, climate change—as long as AI got powerful enough, everything would be solved.\nNow, that dream broke.\nPeople discovered:\n\nCurrent AI can’t replace doctors for complex diagnoses\nCan’t provide truly personalized education\nCan’t solve the energy crisis\nCan’t make work more meaningful\n\nThose promised “AI will give you a better life” felt deceived.\nUnemployment Wave and Anti-AI MovementWorse, while AI stopped evolving, the unemployment it caused was real.\nCustomer service, junior programmers, content moderators, data entry clerks—these jobs were already largely replaced by AI. But the “new job opportunities” AI promised didn’t materialize, because that required stronger AI.\nThe result: a large group lost their jobs without gaining new opportunities.\nThe anti-AI movement began. Not opposing AI itself, but opposing “promising the future with AI while creating present unemployment.”\nSome radical groups began sabotaging AI infrastructure. Data centers were attacked, cloud services forced to enhance security.\nNew Cold War Between NationsGeopolitics became more complex.\nCountries that lagged in the AI race suddenly found catch-up possibilities vanished. If technology stops advancing, first-mover advantage becomes permanent advantage.\nUS-China tech competition became “stock competition”: fighting for AI talent, controlling training data, monopolizing inference compute.\nEurope tried to balance through regulation, but found itself with neither top-tier AI nor rule-making power.\nSome developing countries began considering: should we completely abandon the AI track and focus on other technologies?\nThird Wave: Philosophy and FaithCollapse of ProgressivismThe crisis’s deepest impact was shaking modern society’s core belief: technology will keep advancing.\nFor three hundred years, human society was built on this assumption: today is better than yesterday, tomorrow will be better than today. Steam engines, electricity, computers, internet—each technological revolution validated this assumption.\nAI’s stagnation made people question for the first time: is progress inevitable?\nIf AI hit a ceiling, might other technologies also have one? Physics, biology, materials science—do they all have their own limits?\nMeaning CrisisFor individuals, the impact was more direct.\nMany people, especially young ones, anchored their life’s meaning in “participating in the AI revolution.” They believed they were creating the future, changing the world.\nNow, that narrative collapsed.\nA former OpenAI engineer wrote on their blog:\n\n“I thought I was building stairs to AGI. Now I realize I was just going deeper into a dead end. Those late nights, those sacrificed weekends, those abandoned relationships—for what?”\n\nThis wasn’t isolated. Silicon Valley therapists reported surging depression and anxiety cases.\nNew Currents of ThoughtBut crisis also birthed new thinking.\nSome philosophers began re-examining technology-human relationships. If AI can’t replace humans, what is humanity’s unique value?\nReligious organizations saw long-absent growth. When technology couldn’t provide answers, people turned to traditional meaning sources.\nOthers began exploring “post-AI era” lifestyles: reducing technology dependence, rebuilding community connections, pursuing spiritual fulfillment rather than efficiency improvement.\nChapter Four: Adaptation and TransformationPragmatists WinAmid chaos, some people calmed down.\nThey realized: while AI stopped evolving, existing capabilities were already powerful. The problem wasn’t AI being insufficient, but us not using it well enough.\nA batch of “application-oriented” startups emerged. They didn’t chase breakthroughs, but focused on:\n\nIntegrating AI into traditional industries\nOptimizing existing AI deployment and operations\nDeveloping solutions for specific scenarios\n\nThese companies weren’t sexy, wouldn’t make headlines, but they survived—and thrived.\nEducation ReshapingUniversities began adjusting curricula. Since AI won’t evolve further, fewer research-oriented talents were needed.\nNew training directions:\n\nAI Application Engineers: understand business, can deploy, can optimize\nHuman-AI Collaboration Specialists: design workflows for humans and AI working together\nAI Ethics and Policy: handle AI’s social issues\n\nSkills traditionally thought to be “replaced by AI” regained respect: creativity, empathy, complex communication, ethical judgment.\nRebuilding Social ContractGovernments intervened. Not to push technological progress (that was impossible), but to handle social problems from technological stagnation.\nSome countries experimented with:\n\nBasic Income Pilots: providing basic livelihood for unemployed\nJob Sharing Programs: reducing work hours, creating more positions\nSkills Retraining Projects: helping AI-displaced workers transition\n\nThese attempts weren’t perfect, but at least sought solutions.\nNew Innovation DirectionsInterestingly, AI’s stagnation actually freed innovation in other fields.\nCapital and talent flowed from AI toward:\n\nBiotechnology: gene editing, synthetic biology\nQuantum Computing: though slow progress, gained more attention\nClean Energy: solar, fusion, energy storage technology\nSpace Exploration: commercial spaceflight, lunar bases\n\nIronically, AI once sucked up all resources. Now its stagnation created opportunities for other technologies.\nChapter Five: New EquilibriumBy 2030, the world slowly adapted to the reality of “AI no longer advancing.”\nIndustry LandscapeThe AI industry split into two types:\n\nInfrastructure Companies: providing stable, reliable AI services, like power companies\nApplication Companies: deeply cultivating specific scenarios, pursuing ultimate user experience\n\nThose still trying to “burn money on R&amp;D” either died or pivoted.\nMarkets no longer chased “disruptive innovation” but valued “sustainable profitability.” Boring, but healthy.\nSocial MindsetPeople no longer viewed AI as savior or demon, but as a tool.\nA useful tool, but not omnipotent.\nYoung people’s career choices became more diverse. Not all smart people rushed into tech—doctors, teachers, artists, craftspeople regained respect.\nFuture expectations became more pragmatic. No longer fantasizing “AI will solve everything,” but pragmatically solving concrete problems.\nNew NarrativeA new cultural narrative emerged: technology is limited, humans are unlimited.\nAI hit a ceiling, but human creativity, imagination, emotion, values have no ceiling.\nSci-fi themes changed. No longer “AI rules the world” or “AI saves the world,” but “how humans create meaning under limited technical conditions.”\nPhilosophers posed new questions: if technology can’t provide answers, what can?\nEpilogue: Crisis or Opportunity?Looking back, 2028’s “intelligence crisis” might not have been disaster, but correction.\nIt shattered unrealistic fantasies, forcing humanity to rethink: what do we truly need? What is technology’s meaning? How do we define progress?\nSome say this was humanity’s first time actively “slowing down.” Not because we didn’t want to advance, but because we realized the direction might be wrong.\nPerhaps the real crisis wasn’t technical stagnation, but that we once placed too much hope in technology.\nPerhaps the real opportunity wasn’t finding the next technical breakthrough, but learning to create infinity within limits.\n\nAfterword\nThis is a thought experiment, not prophecy.\nWill 2028 really see such a crisis? I don’t know.\nBut worth thinking about: if it really happens, are we ready?\nMore importantly: should we start thinking about these questions before the crisis arrives?\n\nWhile writing this, GPT-5 just launched. All metrics are improving.\nBut I can’t help wondering: what if one day, all this stops?\nPerhaps this “what if” itself is the most worthy question to consider.\n","categories":["English"],"tags":["AI Future","Tech Prediction","Social Change","Crisis Response"]},{"title":"How to Give Your Agent Eyes and Hands","url":"/2026/02/28/agent-eyes-and-hands-en/","content":"An Assistant That Talks But Doesn’t DoHave you ever been in this situation: you ask an AI a question, it gives you a perfect answer, but you have to execute every single step yourself.\n“Fix this bug for me.”\nIt tells you: open this file, find line 42, change foo to bar, then run npm test to verify.\nSounds great. But it doesn’t lift a finger.\nIt’s like hiring a consultant who sits next to you pointing at the screen but never touches the keyboard. You open the editor, find the line, make the change, run the test, check the result, then report back: “Done, but the test still fails.” Then it gives you the next suggestion.\nAfter a few rounds of this, you start thinking: can’t you just do it yourself?\nThat’s why Agents need “eyes” and “hands” — not just language ability, but the ability to perceive the world and change it.\nEyes: Letting Agents See the WorldAn Agent without perception is like a blindfolded person. You have to constantly narrate the surroundings for it to give advice. Extremely inefficient, and the information you describe is always lossy.\nThe Browser Is the Agent’s EyesIn my recent practice, I connected an Agent to a browser tool. What can it do?\n\nOpen web pages: Visit URLs directly, see page content\nGet snapshots: Obtain the page’s accessibility tree — a structured description far more AI-friendly than screenshots\nTake screenshots: When visual judgment is needed, capture the screen directly\nExecute JavaScript: Run code in the page context to get DOM info or trigger actions\n\nThe most critical one here is Snapshot.\n\nMany people’s first instinct is to give the Agent a screenshot and let a multimodal model “see” the image. That works, but it’s inefficient — a single screenshot might consume thousands of tokens, and the model’s accuracy at extracting structured information from images is far lower than reading structured data directly.\nThe accessibility tree is maintained by the browser for accessibility features. It describes every interactive element’s role, name, and state. For an Agent, this is a “semantic map” — it doesn’t need to know what color a button is or where it sits on screen. It just needs to know “there’s a button called ‘Submit’ with ref e42.”\nSnapshots are AI-friendly. Screenshots are human-friendly. Give Agents snapshots, show humans screenshots.\nBeyond the BrowserEyes aren’t just browsers. An Agent’s perception can extend across many dimensions:\n\nFile system: Read code files, config files, log files\nTerminal output: See stdout and stderr after executing commands\nAPI responses: Parse return data after calling interfaces\nGit status: Know the current branch, uncommitted changes, recent commits\n\nEach perception channel tells the Agent: what the world looks like right now.\nHands: Letting Agents Change the WorldSeeing isn’t enough. They need to act.\nScript Execution Is the Most Universal “Hand”If I could give an Agent only one action capability, I’d choose shell script execution.\nWhy? Because shell is the universal glue. You can use it to:\n\nCreate, modify, delete files\nInstall dependencies, run builds, execute tests\nCall APIs, download resources, process data\nManage Git, deploy code, handle processes\n\nAn Agent that can execute shell scripts can theoretically do anything a programmer can do.\nBut Coarse-Grained Isn’t EnoughPure shell has a problem: it’s too low-level. Having an Agent use sed for text replacement often goes wrong due to escape characters and regex edge cases.\nSo the better approach is to provide multi-layered action capabilities:\n\n\n\nLayer\nTool\nUse Case\n\n\n\nFine-grained\nFile read&#x2F;write, search &amp; replace\nModify code, update configs\n\n\nMedium\nBrowser interaction (click, fill, navigate)\nWeb operations, visual verification\n\n\nCoarse-grained\nShell scripts\nBuild, deploy, system admin\n\n\nFine-grained operations reduce error rates. Coarse-grained operations ensure flexibility. Combined, the Agent can be both stable and fast.\nA Real ExampleWhen I had an Agent help me build this blog, its “hands” collaborated like this:\n\nWrite files (fine-grained): Create Markdown articles, modify config files\nExecute scripts (coarse-grained): hexo generate to build, git push to deploy\nBrowser operations (medium): Open the deployed page, check rendering\nSearch &amp; replace (fine-grained): Found a CSS issue, precisely modified the stylesheet\n\nThe entire workflow ran on its own. I just refreshed the page at the end to see the result.\nEye-Hand Coordination: The Perception-Decision-Action LoopEyes and hands alone are meaningless. The key is forming a closed loop.\n\nPerceive (what do I see?) → Decide (what should I do?) → Act (do it) → Perceive (did the world change?) → ...\n\nThis loop sounds simple, but implementation has many nuances:\n1. Always Verify After ActingAfter an Agent executes an operation, it can’t just assume success. It needs to look back:\n\nChanged code? Run the tests.\nDeployed a website? Open the browser and check.\nInstalled dependencies? Verify node_modules exists.\n\nUnverified actions are dangerous. It’s like crossing the street with your eyes closed — you took the step, but you don’t know if there’s a car coming.\n2. Errors Are Information, Not Dead EndsWhen an Agent’s script throws an error, the error message itself is the most valuable perception input. A good Agent will:\n\nRead the error message\nAnalyze the cause\nAdjust the approach\nRe-execute\n\nRather than just telling you “execution failed, please handle manually.”\n3. Know When to StopThe loop can’t spin forever. The Agent needs to judge:\n\nIs the task complete?\nAm I stuck in an infinite loop?\nShould I ask the user for confirmation?\n\nThis is a kind of metacognition — not just doing things, but knowing what you’re doing and how well you’re doing it.\nCurrent LimitationsHaving talked about the benefits, let’s discuss the real-world pitfalls.\nBlurry Security BoundariesAn Agent that can execute shell scripts can theoretically rm -rf /. While no sane Agent would do this, permission control is a problem that must be taken seriously.\nCurrent approaches typically include:\n\nRestricting the working directory\nBlocking dangerous commands\nRequiring human confirmation for critical operations\n\nBut these are all patches. A more fundamental solution might be sandboxed execution environments — the Agent operates in an isolated container, so even mistakes don’t affect the host system.\nLimited Perception BandwidthEven with browsers and file systems, an Agent’s perception bandwidth is still far below a human’s. A human can instantly see “this page layout is wrong.” An Agent needs to parse the entire DOM tree to reach a similar conclusion.\nMultimodal models are improving, but they’re not at the “glance and understand” level yet. The current best practice is combining structured perception (snapshots) with visual perception (screenshots).\nContext Loss in Long TasksA complex task might require dozens of steps. As steps accumulate, early perception information gets pushed out of the context window. The Agent might forget what it saw three steps ago.\nThis circles back to memory management — what the eyes see also needs to be remembered.\nThe Future: Richer Perception and More Precise ActionI believe Agent eyes and hands will evolve along several directions:\nPerception Side\nReal-time visual understanding: Not just screenshots, but “seeing” the screen like a human — understanding layout, color, animation\nMulti-source information fusion: Simultaneously processing code, logs, browser, database, and other information sources\nActive exploration: Not waiting to be told where to look, but proactively browsing files, checking logs, searching docs\n\nAction Side\nFiner operations: Operating an IDE like a human — refactoring code, running debuggers, setting breakpoints\nCross-system orchestration: Simultaneously operating multiple services, environments, and toolchains\nPhysical world interaction: Extending from the digital world to the physical world through IoT devices and robot interfaces\n\nCoordination Side\nAdaptive strategies: Automatically choosing perception precision and action granularity based on task complexity\nParallel operations: Executing multiple subtasks simultaneously instead of waiting serially\nCollaboration: Multiple Agents dividing work — one handles frontend, one handles backend, one handles testing\n\nFinal ThoughtsBack to the original question: how do you give an Agent its own eyes and hands?\nTechnically, the answer is connecting it to browsers, file systems, terminals, APIs, and other tools. But the deeper answer is: let it form a perception-decision-action loop, and continuously learn and improve within that loop.\nAn Agent that can only talk is a consultant. An Agent that can see and do is a colleague. An Agent that can see, do, and learn from mistakes is a true partner.\nWe’re moving from the “consultant era” to the “colleague era.” The road is long, but every step is exciting.\n\nThe building, deployment, and debugging of this article was entirely done by an Agent with eyes and hands. It saw the page go white, investigated the cause on its own, fixed the config, and redeployed. That’s the power of eyes and hands.\nIf you’re also exploring the boundaries of Agent capabilities, find me on Twitter.\n","categories":["English"],"tags":["AI","Agent","Thoughts","Writing","English"]},{"title":"如何让Agent长出自己的眼和手","url":"/2026/02/28/agent-eyes-and-hands/","content":"一个会说但不会做的助手你有没有遇到过这种情况：你问 AI 一个问题，它给你一段完美的回答，但你需要自己去执行每一步。\n“帮我把这个 bug 修了。”\n它会告诉你：打开某个文件，找到第 42 行，把 foo 改成 bar，然后运行 npm test 验证。\n说得头头是道。但它自己不动手。\n这就像你雇了一个顾问，他坐在旁边指点江山，但从来不碰键盘。你得自己打开编辑器、找到那一行、改完、跑测试、看结果、再回来告诉他”改完了，但测试还是挂了”。然后他再给你下一步建议。\n来回折腾几次，你就会想：你能不能自己来？\n这就是为什么 Agent 需要”眼”和”手”——不只是语言能力，还需要感知世界的能力和改变世界的能力。\n眼：让 Agent 看见世界一个没有感知能力的 Agent，就像一个蒙着眼睛的人。你得不停地口述周围的环境，它才能给出建议。效率极低，而且你描述的信息永远是有损的。\n浏览器就是 Agent 的眼睛我最近的实践中，给 Agent 接入了一个浏览器工具。它能做什么呢？\n\n打开网页：直接访问 URL，看到页面内容\n获取快照（Snapshot）：拿到页面的 accessibility tree——一种结构化的页面描述，比截图更适合 AI 理解\n截图：当需要视觉判断时，直接截取屏幕画面\n执行 JavaScript：在页面上下文中运行代码，获取 DOM 信息或触发操作\n\n这里面最关键的是 Snapshot。\n\n很多人第一反应是给 Agent 截图，让多模态模型”看”图片。这当然可以，但效率很低——一张截图可能要消耗几千 token，而且模型从图片中提取结构化信息的准确率远不如直接读结构化数据。\nAccessibility tree 是浏览器为无障碍功能维护的一棵树，它描述了页面上每个可交互元素的角色、名称和状态。对 Agent 来说，这就是一张”语义地图”——它不需要知道按钮是什么颜色、在屏幕哪个位置，它只需要知道”这里有一个叫’提交’的按钮，ref 是 e42”。\nSnapshot 是 AI 友好的，截图是人类友好的。 给 Agent 用 Snapshot，给人类看截图。\n不只是浏览器眼睛不只是浏览器。Agent 的感知能力可以扩展到很多维度：\n\n文件系统：读取代码文件、配置文件、日志文件\n终端输出：执行命令后看到 stdout 和 stderr\nAPI 响应：调用接口后解析返回数据\nGit 状态：知道当前分支、未提交的改动、最近的 commit\n\n每一种感知通道都在告诉 Agent：世界现在是什么样子的。\n手：让 Agent 改变世界光看不够，还得能动手。\n脚本执行是最通用的”手”如果只能给 Agent 一种行动能力，我会选执行 shell 脚本。\n为什么？因为 shell 是万能胶水。你能用它：\n\n创建、修改、删除文件\n安装依赖、运行构建、执行测试\n调用 API、下载资源、处理数据\n操作 Git、部署代码、管理进程\n\n一个能执行 shell 脚本的 Agent，理论上能做任何程序员能做的事。\n但粗粒度不够纯 shell 有个问题：它太底层了。让 Agent 用 sed 做文本替换，经常会因为转义字符、正则表达式的边界情况而翻车。\n所以更好的做法是提供多层次的行动能力：\n\n\n\n层次\n工具\n适用场景\n\n\n\n精细操作\n文件读写、搜索替换\n修改代码、更新配置\n\n\n中等操作\n浏览器交互（点击、填写、导航）\nWeb 操作、测试验证\n\n\n粗粒度操作\nShell 脚本\n构建、部署、系统管理\n\n\n精细操作减少出错概率，粗粒度操作保证灵活性。两者结合，Agent 才能既稳又快。\n一个真实的例子我让 Agent 帮我搭这个博客的时候，它的”手”是这样协作的：\n\n写文件（精细操作）：创建 Markdown 文章、修改配置文件\n执行脚本（粗粒度）：hexo generate 构建、git push 部署\n浏览器操作（中等操作）：打开部署后的页面，检查渲染效果\n搜索替换（精细操作）：发现 CSS 问题后，精准修改样式文件\n\n整个流程它自己跑完，我只需要最后刷新页面看效果。\n眼手协调：感知-决策-行动的闭环眼和手单独存在没有意义，关键是它们要形成闭环。\n\n感知（看到了什么）→ 决策（应该做什么）→ 行动（去做）→ 感知（做完后世界变了吗）→ ...\n\n这个闭环听起来简单，但实现起来有很多细节：\n1. 行动后必须验证Agent 执行了一个操作后，不能就假设成功了。它需要回头看一眼：\n\n改了代码？跑一下测试。\n部署了网站？打开浏览器看看。\n安装了依赖？检查 node_modules 是否存在。\n\n不验证的行动是危险的。 这就像闭着眼睛过马路——你迈出了步子，但不知道有没有车。\n2. 错误是信息，不是终点当 Agent 执行脚本报错时，错误信息本身就是最有价值的感知输入。一个好的 Agent 会：\n\n读取错误信息\n分析原因\n调整方案\n重新执行\n\n而不是直接告诉你”执行失败了，请手动处理”。\n3. 知道什么时候该停闭环不能无限转下去。Agent 需要判断：\n\n任务完成了吗？\n是不是陷入了死循环？\n是不是应该问用户确认？\n\n这是一种元认知能力——不只是做事，还要知道自己在做什么、做得怎么样。\n当前方案的局限说了这么多好处，也得聊聊现实中的坑。\n安全边界模糊Agent 能执行 shell 脚本，意味着它理论上能 rm -rf /。虽然没有哪个正常的 Agent 会这么做，但权限控制是一个必须认真对待的问题。\n目前的做法通常是：\n\n限制工作目录\n禁止危险命令\n关键操作需要人工确认\n\n但这些都是打补丁。更根本的解决方案可能是沙箱化执行环境——Agent 在一个隔离的容器里操作，即使出错也不会影响宿主系统。\n感知带宽有限即使有了浏览器和文件系统，Agent 的感知带宽仍然远低于人类。人类一眼就能看出”这个页面布局不对”，Agent 需要解析整个 DOM 树才能得出类似的判断。\n多模态模型在进步，但还没到”一眼看懂”的程度。 目前的最佳实践是结构化感知（Snapshot）和视觉感知（截图）结合使用。\n长任务的上下文丢失一个复杂任务可能需要几十步操作。随着步骤增多，早期的感知信息会被挤出上下文窗口。Agent 可能忘了自己三步之前看到了什么。\n这又回到了记忆管理的问题——眼睛看到的东西，也需要被记住。\n未来：更丰富的感知和更精准的行动我觉得 Agent 的眼和手会沿着几个方向进化：\n感知侧\n实时视觉理解：不只是截图，而是像人一样”看”屏幕，理解布局、颜色、动画\n多源信息融合：同时处理代码、日志、浏览器、数据库等多个信息源\n主动探索：不等你告诉它看哪里，自己去翻文件、查日志、搜文档\n\n行动侧\n更精细的操作：像人一样操作 IDE——重构代码、运行调试器、设置断点\n跨系统协作：同时操作多个服务、多个环境、多个工具链\n物理世界交互：通过 IoT 设备、机器人接口，从数字世界延伸到物理世界\n\n协调侧\n自适应策略：根据任务复杂度自动选择感知精度和行动粒度\n并行操作：同时执行多个子任务，而不是串行等待\n协作能力：多个 Agent 分工合作，一个负责前端，一个负责后端，一个负责测试\n\n写在最后回到最初的问题：如何让 Agent 长出自己的眼和手？\n技术上，答案是给它接入浏览器、文件系统、终端、API 等工具。但更深层的答案是：让它形成感知-决策-行动的闭环，并且在这个闭环中不断学习和改进。\n一个只会说的 Agent 是顾问。一个能看能做的 Agent 是同事。一个能看、能做、还能从错误中学习的 Agent，才是真正的伙伴。\n我们正在从”顾问时代”走向”同事时代”。这条路还很长，但每一步都让人兴奋。\n\n这篇文章的搭建、部署、调试过程，全部由一个有眼有手的 Agent 完成。它看到了页面白屏，自己查了原因，改了配置，重新部署。这就是眼和手的力量。\n如果你也在探索 Agent 的能力边界，欢迎在 Twitter 上找我聊。\n","categories":["技术"],"tags":["AI","Agent","技术思考"]},{"title":"谈谈超级大模型时代的Agent记忆管理","url":"/2026/02/28/agent-memory-management/","content":"一个真实的场景最近我一直在用一个 AI Agent 帮我干活——搭博客、写组件库、调样式、部署代码。几天下来，我发现一个有意思的现象：\n它记得我喜欢用 pnpm，记得我的 GitHub 用户名，记得我说过”顶部大色块不要了”，甚至记得我微信号是什么。\n但有时候它又会犯一些低级错误，比如把已经修过的 bug 再改回去，或者忘了我之前明确说过的偏好。\n这让我开始认真思考一个问题：在大模型上下文窗口已经动辄几十万 token 的今天，Agent 的记忆管理到底应该怎么做？\n上下文窗口够大，就不需要记忆了吗？很多人有一个直觉：模型的上下文窗口越来越大，128K、200K、甚至 1M token，是不是把所有历史对话塞进去就完事了？\n答案是：远远不够。\n原因有三个：\n1. 成本问题是硬约束即使模型支持 1M token 的上下文，你真的要每次推理都把所有历史塞进去吗？按照目前的 API 定价，一次百万 token 的推理调用，成本可能就是几块钱。一个活跃的 Agent 一天可能要推理几百次。\n算一笔账：如果每次都用满 1M 上下文，一天的 API 费用可能就要上千。 这对个人开发者来说完全不可接受，对企业来说也是巨大的成本压力。\n2. 长上下文 ≠ 长记忆这是很多人忽略的一点。模型在处理超长上下文时，对中间部分的信息提取能力是显著下降的——这就是所谓的 “Lost in the Middle” 问题。\n你把三个月前的一段对话塞在 50 万 token 的中间位置，模型大概率会”视而不见”。上下文窗口是个队列，不是数据库。你不能指望模型像搜索引擎一样精准地从海量文本中定位关键信息。\n3. 不是所有信息都值得记住这一点最关键。人类的记忆系统之所以高效，不是因为我们记住了一切，而是因为我们善于遗忘。\n你不需要记住每一次 git push 的输出日志，但你需要记住”这个项目用 pnpm 不用 npm，因为 npm 有缓存权限问题”。前者是噪音，后者是知识。\n好的记忆管理，本质上是一门遗忘的艺术。\n当前主流的 Agent 记忆方案目前业界的 Agent 记忆管理大致分几个层次：\n\n工作记忆（Working Memory）就是当前对话的上下文。模型直接能”看到”的部分。容量有限，但访问速度最快、准确度最高。\n类比人类：你正在思考的事情。\n短期记忆（Short-term Memory）最近几轮对话的摘要。通常通过 LLM 自动总结压缩，保留关键信息，丢弃细节。\n类比人类：你今天做了什么，大致还记得，但具体每句话说了什么已经模糊了。\n长期记忆（Long-term Memory）跨会话持久化的信息。通常存储在向量数据库中，通过 embedding 检索相关内容。\n类比人类：你知道某个同事的习惯、某个项目的架构决策——这些是长期积累的知识。\n外部知识库（External Knowledge）文档、代码库、API 文档等。Agent 通过 RAG（检索增强生成）按需获取。\n类比人类：你不需要背下整本手册，但你知道去哪里查。\n我观察到的几个关键问题在实际使用 Agent 的过程中，我发现现有的记忆方案有几个痛点：\n问题一：摘要会丢失关键细节当 Agent 把长对话压缩成摘要时，它必须做取舍。但什么是重要的，什么是不重要的，这个判断本身就很难。\n举个例子：我跟 Agent 说”图片路径不要加 /ideas/ 前缀”。这句话在一段很长的调试对话中可能只占一行，但它是一个关键的项目规则。如果摘要时被丢掉了，下次它又会犯同样的错误。\n问题二：向量检索的召回率不稳定长期记忆通常依赖向量相似度检索。但自然语言的语义是模糊的——“npm 有权限问题”和”用 pnpm 代替 npm”，语义上相关但表述差异很大。检索时可能漏掉关键信息。\n问题三：记忆缺乏结构化大多数 Agent 的记忆就是一堆文本片段。但人类的记忆是有结构的——我们会把知识组织成概念、规则、经验、偏好等不同类别。\n一个 Agent 应该知道”用户偏好 pnpm”是一条偏好规则，而不只是某次对话中出现过的一句话。\n我的思考：理想的 Agent 记忆系统基于这些观察，我觉得未来的 Agent 记忆系统应该有几个特征：\n\n1. 分层 + 分类不是简单的”短期&#x2F;长期”二分法，而是按信息类型分类：\n\n\n\n类型\n示例\n特征\n\n\n\n事实\n用户的 GitHub 用户名是 hongqi-lgs\n确定性高，几乎不变\n\n\n偏好\n用户喜欢 headed 模式的浏览器\n可能变化，但变化频率低\n\n\n规则\n图片路径不加 &#x2F;ideas&#x2F; 前缀\n项目级别的硬约束\n\n\n经验\nnpm 有缓存权限问题，用 pnpm 绕过\n从错误中学到的教训\n\n\n状态\n当前正在开发 Switch 组件\n时效性强，会过期\n\n\n不同类型的记忆，应该有不同的存储策略、检索权重和过期机制。\n2. 主动遗忘Agent 应该有能力主动清理过时的信息。三个月前的调试日志、已经解决的 bug 详情、临时的中间状态——这些都应该被逐步淘汰。\n不是删除，而是降权。 就像人类的记忆一样，不常被提起的信息会逐渐模糊，但如果被触发，还是能想起来。\n3. 记忆的自我修正当 Agent 发现自己的记忆与现实矛盾时，应该能自动更新。比如用户说”我现在改用 npm 了”，Agent 不应该还抱着”用 pnpm”的旧记忆不放。\n这需要一个冲突检测和解决机制——新信息和旧记忆矛盾时，以新信息为准，并标记旧记忆为”已过时”。\n4. 可解释的记忆用户应该能看到 Agent 记住了什么、为什么记住、什么时候记住的。这不仅是透明度的问题，也是信任的基础。\n如果你不知道 AI 记住了你的什么信息，你怎么敢把重要的事情交给它？\n一个更大胆的想法我最近在想一个可能有点超前的观点：\n未来的 Agent 记忆，可能不应该是”存储-检索”模式，而应该是”生长-进化”模式。\n\n什么意思呢？\n现在的记忆系统本质上是一个数据库：存进去，查出来。但人类的记忆不是这样工作的。我们的记忆会在睡眠中被重新整理，不同的记忆片段会被重新关联，形成新的理解。\n想象一下：一个 Agent 在”空闲时间”（没有用户交互的时候），自动回顾自己的记忆，把零散的经验整理成系统的知识，发现不同项目之间的共性模式，甚至主动提出优化建议。\n这不再是记忆管理，而是知识进化。\n当然，这需要解决很多技术问题——计算成本、幻觉控制、知识一致性验证等等。但我相信这是 Agent 发展的一个重要方向。\n写在最后我们正处在一个有趣的时间节点。大模型的能力在飞速提升，上下文窗口在不断扩大，但 Agent 的记忆管理仍然是一个远未解决的问题。\n上下文窗口再大，也只是给了你一个更大的工作台。真正的智能，在于知道工作台上该放什么、不该放什么。\n作为一个每天都在和 Agent 打交道的人，我越来越觉得：记忆管理可能是决定 Agent 能否从”工具”进化为”伙伴”的关键一步。\n一个记不住你偏好的助手，永远只是一个需要反复调教的工具。一个能理解你、记住你、甚至能预判你需求的助手，才是真正的伙伴。\n我们还在路上，但方向已经清晰了。\n\n如果你也在做 Agent 相关的工作，欢迎交流。我的 Twitter 是 @xiaosen_lu。\n","categories":["技术"],"tags":["AI","Agent","技术思考"]},{"title":"Agent Memory Management in the Era of Super Large Models","url":"/2026/02/28/agent-memory-management-en/","content":"A Real ScenarioI’ve been using an AI Agent recently to help me with all sorts of tasks — building a blog, writing a component library, tweaking styles, deploying code. After a few days, I noticed something interesting:\nIt remembers that I prefer pnpm, knows my GitHub username, remembers I said “get rid of the top banner,” and even knows my WeChat ID.\nBut sometimes it makes rookie mistakes — like reverting a bug fix I’d already confirmed, or forgetting a preference I’d explicitly stated.\nThis got me thinking seriously about a question: In an era where model context windows are already hundreds of thousands of tokens, how should Agent memory management actually work?\nIs a Bigger Context Window Enough?Many people have an intuition: context windows are getting huge — 128K, 200K, even 1M tokens. Can’t we just stuff all the history in there and call it a day?\nThe answer is: not even close.\nThree reasons:\n1. Cost Is a Hard ConstraintEven if a model supports 1M tokens of context, do you really want to fill it every single inference call? At current API pricing, a single million-token call might cost several dollars. An active Agent might run hundreds of inferences per day.\nDo the math: if you max out 1M context every time, daily API costs could easily hit thousands of dollars. That’s unacceptable for individual developers and a massive cost pressure for enterprises.\n2. Long Context ≠ Long MemoryThis is something many people overlook. Models show significantly degraded information extraction from the middle portions of very long contexts — the so-called “Lost in the Middle” problem.\nIf you bury a conversation from three months ago at the 500K-token mark, the model will likely ignore it completely. A context window is a queue, not a database. You can’t expect a model to pinpoint critical information from massive text like a search engine.\n3. Not Everything Is Worth RememberingThis is the most crucial point. The reason human memory is efficient isn’t because we remember everything — it’s because we’re good at forgetting.\nYou don’t need to remember the output of every git push, but you do need to remember “this project uses pnpm instead of npm because npm has cache permission issues.” The former is noise; the latter is knowledge.\nGood memory management is essentially the art of forgetting.\nCurrent Mainstream Agent Memory ApproachesThe industry’s Agent memory management roughly breaks down into several layers:\n\nWorking MemoryThe current conversation context. What the model can directly “see.” Limited capacity, but fastest access and highest accuracy.\nHuman analogy: what you’re actively thinking about right now.\nShort-term MemorySummaries of recent conversation rounds. Usually auto-compressed by an LLM, retaining key information while discarding details.\nHuman analogy: you roughly remember what you did today, but the exact words of every conversation are already fuzzy.\nLong-term MemoryPersistent information across sessions. Typically stored in vector databases, retrieved via embedding similarity.\nHuman analogy: you know a colleague’s habits, a project’s architectural decisions — knowledge accumulated over time.\nExternal KnowledgeDocumentation, codebases, API docs, etc. Agents access these on-demand through RAG (Retrieval-Augmented Generation).\nHuman analogy: you don’t memorize the entire manual, but you know where to look.\nKey Problems I’ve ObservedThrough hands-on Agent usage, I’ve identified several pain points with existing memory approaches:\nProblem 1: Summaries Lose Critical DetailsWhen an Agent compresses long conversations into summaries, it has to make trade-offs. But judging what’s important and what’s not is inherently difficult.\nExample: I told my Agent “don’t add the /ideas/ prefix to image paths.” In a long debugging conversation, this might be just one line, but it’s a critical project rule. If it gets dropped during summarization, the same mistake will happen again.\nProblem 2: Vector Retrieval Has Unstable RecallLong-term memory typically relies on vector similarity search. But natural language semantics are fuzzy — “npm has permission issues” and “use pnpm instead of npm” are semantically related but expressed very differently. Critical information might be missed during retrieval.\nProblem 3: Memory Lacks StructureMost Agent memories are just piles of text fragments. But human memory is structured — we organize knowledge into concepts, rules, experiences, preferences, and other categories.\nAn Agent should know that “user prefers pnpm” is a preference rule, not just a sentence that appeared in some conversation.\nMy Vision: The Ideal Agent Memory SystemBased on these observations, I believe future Agent memory systems should have several characteristics:\n\n1. Layered + CategorizedNot a simple “short-term&#x2F;long-term” binary, but categorized by information type:\n\n\n\nType\nExample\nCharacteristics\n\n\n\nFacts\nUser’s GitHub username is hongqi-lgs\nHigh certainty, rarely changes\n\n\nPreferences\nUser likes headed browser mode\nMay change, but infrequently\n\n\nRules\nDon’t add &#x2F;ideas&#x2F; prefix to image paths\nProject-level hard constraints\n\n\nLessons\nnpm has cache permission issues, use pnpm\nLearned from mistakes\n\n\nState\nCurrently developing the Switch component\nTime-sensitive, will expire\n\n\nDifferent types of memory should have different storage strategies, retrieval weights, and expiration mechanisms.\n2. Active ForgettingAgents should be able to proactively clean up outdated information. Debugging logs from three months ago, details of resolved bugs, temporary intermediate states — these should all be gradually phased out.\nNot deletion, but de-prioritization. Like human memory, information that’s rarely recalled gradually fades, but can still be retrieved if triggered.\n3. Self-Correcting MemoryWhen an Agent discovers its memory contradicts reality, it should auto-update. If a user says “I’ve switched to npm now,” the Agent shouldn’t cling to the old “use pnpm” memory.\nThis requires a conflict detection and resolution mechanism — when new information contradicts old memory, prioritize the new and mark the old as “outdated.”\n4. Explainable MemoryUsers should be able to see what the Agent remembers, why it remembers it, and when it was stored. This isn’t just about transparency — it’s the foundation of trust.\nIf you don’t know what information an AI has stored about you, how can you trust it with important tasks?\nA Bolder IdeaI’ve been mulling over a possibly ahead-of-its-time thought:\nFuture Agent memory shouldn’t follow a “store-retrieve” model, but a “grow-evolve” model.\n\nWhat do I mean?\nCurrent memory systems are essentially databases: store it, query it. But human memory doesn’t work that way. Our memories get reorganized during sleep, different memory fragments get reconnected, forming new understanding.\nImagine: an Agent during “idle time” (when there’s no user interaction) automatically reviews its memories, organizes scattered experiences into systematic knowledge, discovers common patterns across different projects, and even proactively suggests optimizations.\nThis is no longer memory management — it’s knowledge evolution.\nOf course, this requires solving many technical challenges — computational cost, hallucination control, knowledge consistency verification, and more. But I believe this is an important direction for Agent development.\nFinal ThoughtsWe’re at a fascinating inflection point. Model capabilities are advancing rapidly, context windows keep expanding, but Agent memory management remains a largely unsolved problem.\nNo matter how large the context window gets, it only gives you a bigger workbench. True intelligence lies in knowing what to put on that workbench — and what to leave off.\nAs someone who works with Agents every day, I increasingly believe that memory management might be the key step in determining whether Agents evolve from “tools” into “partners.”\nAn assistant that can’t remember your preferences will always be a tool that needs constant re-training. An assistant that understands you, remembers you, and can even anticipate your needs — that’s a true partner.\nWe’re still on the journey, but the direction is clear.\n\nIf you’re also working on Agent-related projects, I’d love to connect. Find me on Twitter at @xiaosen_lu.\n","categories":["English"],"tags":["AI","Agent","Thoughts","Writing","English"]},{"title":"2028全球智能危机","url":"/2026/03/02/2028-global-intelligence-crisis/","content":"2028全球智能危机序章：那个周五下午2028年6月的某个周五，硅谷的工程师们发现了一件奇怪的事：所有大模型的benchmark分数不再提升了。\n不是进步变慢，而是完全停止。\n无论怎么增加算力、扩大数据集、调整架构，指标就像撞上了一堵看不见的墙。GPT-7、Claude Opus 5、Gemini Ultra 3.0，所有模型的能力都固定在某个水平，无法突破。\n最初，大家以为是测试方法的问题。但很快，一个更可怕的现实浮现出来：AI的能力上限到了。\n这不是技术路线的问题，而是基础原理的限制。就像你无法让蒸汽机的效率超过卡诺循环的理论极限，当前范式的AI也遇到了它的天花板。\n而这个天花板，比所有人预期的都要低。\n第一波冲击：产业崩塌AI公司的多米诺骨牌消息传出后的第一周，纳斯达克AI板块跌去40%。不是因为AI没用了，而是因为市场突然意识到：这些公司的估值建立在”AI会持续进化”的假设上。\n假设破灭了。\n那些靠”下一代模型会解决现有问题”来吸引投资的创业公司，融资突然断了。VC们开始重新审视估值模型，发现很多公司的商业逻辑根本不成立。\nOpenAI的估值从1500亿美元跌到200亿。不是因为它的产品不好用，而是因为它失去了”未来”。\n大厂的战略调整Google、微软、Meta紧急召开董事会。他们面临一个艰难的选择：\n\n继续投入巨资研发，期待突破？\n还是把资源转向现有技术的应用？\n\n大部分选择了后者。AI研究团队被大规模裁员，只保留维护和优化的最小团队。\n投资人的逻辑很简单：既然技术不会再进步，为什么要养那么多研究员？\n教育行业的恐慌更大的冲击在教育领域。\n过去两年，无数学生涌入AI专业，期待这是通往未来的门票。突然间，这扇门关上了。\n大学慌了。CS专业的申请人数断崖式下降。教授们开始重新思考：如果AI不再进化，我们该教什么？\n更讽刺的是，那些因为”AI会取代我”而转行学AI的传统行业从业者，发现自己既回不去原来的行业，又进不了AI行业。\n第二波冲击：社会撕裂期待落空的愤怒最危险的不是技术停滞，而是期待的破灭。\n过去几年，媒体、专家、企业家们不断告诉大众：”AI会解决所有问题。” 医疗、教育、贫困、气候变化，似乎只要AI足够强大，一切都会迎刃而解。\n现在，这个梦碎了。\n人们发现：\n\n现有的AI无法替代医生做复杂诊断\n无法提供真正个性化的教育\n无法解决能源危机\n无法让工作变得更有意义\n\n那些被承诺”AI会给你更好生活”的人们，感到被欺骗了。\n失业潮与反AI运动更糟糕的是，AI虽然停止进化，但已经造成的失业是真实的。\n客服、初级程序员、内容审核员、数据录入员——这些工作已经大量被AI取代。但AI承诺的”新工作机会”没有出现，因为那需要更强的AI。\n结果就是：一大批人失去了工作，却没有获得新的机会。\n反AI运动开始了。不是反对AI本身，而是反对”用AI承诺未来，却制造现在的失业”。\n一些激进组织开始破坏AI基础设施。数据中心遭到攻击，云服务被迫加强安保。\n国家间的新冷战地缘政治变得更加复杂。\n那些在AI竞赛中落后的国家，突然发现赶超的可能性消失了。如果技术不再进步，先发优势就是永久优势。\n中美之间的科技竞争变成了”存量争夺”：抢夺AI人才、控制训练数据、垄断推理算力。\n欧洲试图通过监管来制衡，但发现自己既没有顶尖的AI，也没有制定规则的话语权。\n一些发展中国家开始考虑：是不是该彻底放弃AI路线，专注于其他技术？\n第三波冲击：哲学与信仰进步主义的崩溃这场危机最深层的影响，是它动摇了现代社会的核心信仰：技术会持续进步。\n过去三百年，人类社会建立在这样一个假设上：今天比昨天好，明天会比今天更好。蒸汽机、电力、计算机、互联网，每一次技术革命都验证了这个假设。\nAI的停滞，第一次让人们质疑：进步是必然的吗？\n如果AI遇到了天花板，其他技术会不会也有？物理学、生物学、材料科学，是不是都有各自的极限？\n意义危机对个体来说，冲击更加直接。\n很多人，尤其是年轻人，把人生意义寄托在”参与AI革命”上。他们相信自己在创造未来，在改变世界。\n现在，这个叙事崩塌了。\n一位前OpenAI工程师在博客中写道：\n\n“我以为自己在建造通往AGI的阶梯。现在我意识到，我只是在一个死胡同里越走越深。那些深夜的加班，那些放弃的周末，那些牺牲的关系——为了什么？”\n\n这不是个例。硅谷的心理咨询师报告，抑郁和焦虑症患者激增。\n新的思潮但危机也催生了新的思考。\n一些哲学家开始重新审视技术与人的关系。如果AI不能替代人类，那人类的独特价值是什么？\n宗教组织迎来了久违的增长。当技术无法提供答案，人们转向传统的意义来源。\n还有一些人开始探索”后AI时代”的生活方式：减少对技术的依赖，重建社区联系，追求精神上的满足而不是效率的提升。\n第四章：适应与转型务实派的胜利在混乱中，一些人开始冷静下来。\n他们意识到：AI虽然停止进化，但现有能力已经很强大了。问题不是AI不够好，而是我们用得不够好。\n一批”应用型”创业公司兴起。他们不追求技术突破，而是专注于：\n\n把AI整合进传统行业\n优化现有AI的部署和运维\n开发针对特定场景的解决方案\n\n这些公司不性感，不会上新闻头条，但它们活下来了，而且活得不错。\n教育的重塑大学开始调整课程。既然AI不会再进化，就不需要那么多研究型人才了。\n新的培养方向是：\n\nAI应用工程师：懂业务、会部署、能优化\n人机协作专家：设计人和AI共同工作的流程\nAI伦理与政策：处理AI带来的社会问题\n\n那些传统被认为”会被AI取代”的技能，重新受到重视：创造力、同理心、复杂沟通、伦理判断。\n社会契约的重建政府开始介入。不是为了推动技术进步（那已经不可能了），而是为了处理技术停滞带来的社会问题。\n一些国家尝试：\n\n基本收入试点：为失业者提供基本生活保障\n工作分享计划：减少工作时长，创造更多岗位\n技能再培训项目：帮助被AI替代的工人转型\n\n这些尝试并不完美，但至少是在寻找解决方案。\n新的创新方向有趣的是，AI的停滞反而释放了其他领域的创新。\n资金和人才从AI流向了：\n\n生物技术：基因编辑、合成生物学\n量子计算：虽然进展缓慢，但有了更多关注\n清洁能源：太阳能、核聚变、储能技术\n太空探索：商业航天、月球基地\n\n讽刺的是，AI曾经吸走了所有资源。现在它的停滞，给其他技术创造了机会。\n第五章：新的平衡到2030年，世界慢慢适应了”AI不再进步”的现实。\n产业格局AI产业分化成两类：\n\n基础设施公司：提供稳定、可靠的AI服务，像电力公司一样\n应用公司：在特定场景中深耕，追求极致的用户体验\n\n那些试图继续”烧钱搞研发”的公司，要么死了，要么转型了。\n市场不再追捧”颠覆式创新”，而是看重”可持续盈利”。这很无聊，但也很健康。\n社会心态人们不再把AI视为救世主或恶魔，而是一种工具。\n有用的工具，但不是万能的。\n年轻人的职业选择更加多元。不是所有聪明人都涌入科技行业了，医生、老师、艺术家、工匠又重新成为受尊敬的职业。\n对未来的期待更加务实。不再幻想”AI会解决一切”，而是脚踏实地地解决具体问题。\n新的叙事一种新的文化叙事出现了：技术有限，人无限。\nAI遇到了天花板，但人的创造力、想象力、情感、价值观没有天花板。\n科幻小说的主题变了。不再是”AI统治世界”或”AI拯救世界”，而是”在有限的技术条件下，人类如何创造意义”。\n哲学家提出新的问题：如果技术不能提供答案，什么能？\n尾声：危机还是转机？回头看，2028年的”智能危机”可能不是一场灾难，而是一次修正。\n它打破了不切实际的幻想，迫使人类重新思考：我们真正需要什么？技术的意义是什么？进步的定义是什么？\n有人说，这是人类历史上第一次主动”放慢脚步”。不是因为我们不想前进，而是因为我们意识到，前进的方向可能错了。\n也许，真正的危机不是技术停滞，而是我们曾经对技术寄予了过高的期望。\n也许，真正的转机不是找到下一个技术突破，而是学会在有限中创造无限。\n\n后记\n这是一个思想实验，不是预言。\n2028年会不会真的发生这样的危机？我不知道。\n但值得思考的是：如果真的发生了，我们准备好了吗？\n更重要的是：我们是不是应该在危机到来之前，就开始思考这些问题？\n\n写这篇文章的时候，GPT-5刚刚发布。所有指标都在提升。\n但我还是忍不住想：如果有一天，这一切停止了呢？\n也许，这个”如果”本身，就是最值得思考的问题。\n","categories":["技术思考"],"tags":["AI未来","技术预测","社会变革","危机应对"]},{"title":"New Company Models in the AI Era","url":"/2026/02/28/ai-company-model-en/","content":"One Person Outperforming a TeamOver the past year or two, I’ve noticed an increasingly obvious trend: many impressive products are built by just one or two people.\nA solo developer, using AI to help write code, design interfaces, craft copy, and handle operations, can ship a complete product in weeks. Three years ago, the same thing would have required a five-to-ten person team working for months.\nThis isn’t an anomaly. Browse Product Hunt’s trending products — more and more are labeled “Solo Founder.” Check GitHub’s popular projects — many have only one or two contributors, yet their code quality and feature completeness rival team efforts.\nWhat happened?\nThe answer is simple: AI changed the productivity equation.\nThe Old Model: Strength in NumbersThe traditional company model is built on a basic assumption: doing more requires more people.\nTo develop a product, you need product managers, designers, frontend engineers, backend engineers, QA engineers, DevOps. To market it, you need marketing, operations, customer service. To manage all these people, you need project managers, HR, admin.\nAs headcount grows, communication costs explode. A five-person team has 10 communication lines. Ten people: 45. Fifty people: 1,225. That’s why large companies are inefficient — it’s not that people are bad, it’s that communication complexity grows exponentially with headcount.\nSo you need hierarchies, processes, meetings, documents, approvals. These things don’t create value themselves, but without them, large organizations spiral out of control. Management overhead becomes a massive hidden tax.\nThe essence of the old model: trading management costs for scale effects.\nThis made sense in the industrial age. Assembly lines needed workers, workers needed management. But in the information age, especially the AI age, this equation is breaking down.\nWhat AI ChangesAI does one thing: it turns work that required “one person” into work that requires “one person for one hour.”\nWriting marketing copy used to take a copywriter a full day. Now you explain the requirements to AI, get a draft in ten minutes, spend twenty minutes polishing.\nCreating a product prototype used to take a designer several days. Now you generate interfaces with AI, tweak them yourself, done in half a day.\nWriting a backend API used to take an engineer half a day plus debugging. Now AI writes it, you review, and within an hour tests are passing.\nThe efficiency gain in individual steps isn’t the point. The point is: when every step speeds up 5-10x, one person can cover the work scope of several people.\nThis doesn’t mean AI replaces people. It means AI dramatically expands one person’s capability boundary. A product-minded engineer, plus AI, can simultaneously play the roles of product manager, designer, frontend, backend, QA, and copywriter. Not 100% in each role, but 70-80% is enough to ship a product.\n\nNew Model One: The Super IndividualThe first new model is the Super Individual — one person is an entire company.\nThis isn’t a new concept; freelancers have always existed. But AI-era super individuals are different. Traditional freelancers could usually only do one thing — designers designed, programmers coded. Now one person can go full-stack: from idea to product to launch to operations, handling the entire chain.\nI’ve seen a solo developer build a SaaS product generating tens of thousands of dollars monthly. His “team”: himself + ChatGPT + Cursor + Midjourney + a few automation tools. Customer service handled by AI chatbot, finances automated through Stripe, deployment one-click via Vercel.\nThe advantages are obvious:\n\nZero communication cost. All decisions happen in one brain. No meetings, no alignment, no waiting for approvals.\nMaximum flexibility. Want to add a feature today? It’s live this afternoon. Want to pivot? Done tomorrow.\nExtremely high margins. No labor costs means revenue nearly equals profit.\n\nThere are limitations, of course: one person’s energy is finite, limiting scale. But the definition of “too big for one person” keeps expanding — things impossible for one person before are now feasible.\nNew Model Two: AI-Native Small TeamsThe second model is the AI-Native small team — three to ten people producing what previously required fifty.\nThe defining characteristic: every person is a full-stack talent, with AI as everyone’s co-pilot.\nTraditional teams divide by function: product group, design group, dev group, QA group. AI-Native teams divide by business module: each person owns a complete module from requirements to launch. AI fills in their skill gaps.\nPatterns I’ve observed:\nVery few meetings. Since everyone can make independent decisions, frequent alignment isn’t needed. Async communication dominates, documentation-driven.\nNo dedicated managers. The team is small enough that “management” isn’t needed. Everyone is a maker; nobody is just a manager.\nAI tools deeply integrated into workflows. Not occasional ChatGPT use, but AI permeating every step — Cursor for coding, AI for design generation, AI-assisted documentation, AI for data analysis.\nHiring criteria changed. It’s no longer about how many programming languages you know or frameworks you’ve used. It’s: can you independently take something from start to finish? Can you efficiently use AI tools? Do you have product sense?\nMany emerging AI startups exemplify this model. You’ll notice that companies producing amazing products often have surprisingly tiny teams.\nNew Model Three: Dynamic NetworksThe third model is more radical: a company is no longer a fixed organization but a dynamic collaboration network.\nThe core team might be just two or three people, responsible for product direction and core technology. Everything else is accomplished through combinations of freelancers, contractors, and AI Agents. Need a marketing campaign? Partner with a freelance marketing expert for two weeks. Need complex data analysis? Let an AI Agent run it, human reviews results.\nThe essence: converting fixed costs to variable costs.\nTraditional companies pay full-time employees regardless of workload. In the dynamic network model, you only pay when needed. AI further reduces friction — the costs of finding contractors, coordinating, communicating, and reviewing used to be high. Now AI handles many intermediate steps.\nImpact on Traditional Large CompaniesWhat do these new models mean for traditional large companies?\nFirst, competitors multiply. Products that only large companies could build before, small teams can now build too. And small teams are faster, more flexible, more willing to take risks. Features that take a large company a year might take a small team two months.\nSecond, talent drain accelerates. The best talent realizes they can create enormous value with just themselves and AI. Why stay at a big company attending meetings, writing weekly reports, waiting for approvals? The super individual and small team models are increasingly attractive to top talent.\nThird, organizational bloat costs more. Previously, large companies being slightly inefficient was fine because competitors were similar. Not anymore — your competitor might be a three-person team with 10x your decision speed and 5x your iteration speed.\nThis doesn’t mean large companies will disappear. They have their advantages: brand, distribution, data, capital, compliance capabilities. But large companies need to transform — either get smaller, get faster, or do things small companies can’t.\n\nNew Competitive MoatsUnder new models, competitive moats are shifting.\nOld moats: More people, more money, deeper tech accumulation. You have a thousand engineers; others can’t catch up.\nNew moats:\n\nData flywheels. Whoever’s product has more users, generates more data, trains better models — that’s the moat.\nAI utilization efficiency. Same AI, but some achieve 10x efficiency gains while others only get 2x. The difference lies in understanding AI’s capability boundaries and workflow design.\nSpeed. In the AI era, speed is a moat. Ship two weeks before competitors and you capture users and feedback data first.\nTaste and judgment. When AI reduces execution costs to near zero, what determines success is “what to build” not “how to build it.” Product taste and strategic judgment matter more than ever.\nTrust and brand. AI can help you build products but can’t build trust. Users choosing you over competitors increasingly depends on brand and reputation.\n\nWhat Future Companies Look LikeExtrapolating these trends, future companies might look like this:\nMost companies will get smaller. Not because they’re doing less, but because the same work requires fewer people. A 50-person company might shrink to 15, with equal or higher output.\nOrganizational structures will flatten. Middle management will compress. When everyone can make independent decisions and AI handles coordination, hierarchies lose their purpose.\nThe boundary between full-time employees and external collaborators will blur. Core team + dynamic external network becomes the norm.\nAI will become “employees.” Not metaphorically — literally. Companies will have AI Agents handling customer service, data analysis, content production, code review. These Agents will have their own “desks” (runtime environments), “permissions” (API access), “performance reviews” (quality monitoring).\nStartup barriers will drop dramatically. A person with an idea, without fundraising or hiring, can build a competitive product. This means startup volume will explode, but success standards will also rise — because competition intensifies.\nFinal ThoughtsEvery technological revolution reshapes organizational forms.\nThe Industrial Revolution spawned factories and assembly lines. The Information Revolution spawned internet companies and remote work. The AI Revolution is spawning new company models — smaller, faster, more flexible, with AI deeply embedded in every function.\nFor individuals, this is both opportunity and challenge. The opportunity: you no longer need a large team to do big things. The challenge: if you don’t learn to collaborate with AI, your competitiveness will decline rapidly.\nMy prediction: in the next five years, “knowing how to use AI” will go from a bonus to a baseline skill, just like “knowing how to use a computer.” It’s not a question of whether you’ll learn, but when you’ll start.\nAnd those who embrace new models earliest will capture the biggest dividends.\nAs with every technological revolution — the early movers feast, the followers get scraps, and the oblivious foot the bill.\n","categories":["English"],"tags":["AI","Thoughts","English","Business"]},{"title":"AI Coding 的绊脚石之一：程序的隐式契约问题","url":"/2026/03/02/ai-coding-implicit-contracts/","content":"AI Coding 的绊脚石之一：程序的隐式契约问题\n引言：当AI遇到”不言而喻”的规则最近在尝试用AI助手编写代码时，我发现了一个有趣的现象：有些代码看起来完美无缺，逻辑清晰，语法正确，但就是无法正常工作。经过深入分析，我发现问题的根源往往不在于代码本身，而在于那些从未被明确写出，却对程序运行至关重要的隐式契约。\n这些隐式契约就像是软件开发中的”潜规则”——人类开发者通过经验和上下文理解它们，但AI却常常在这些规则面前碰壁。\n什么是隐式契约？隐式契约（Implicit Contract）指的是那些没有被明确写在代码或文档中，但对程序正确运行至关重要的假设、约定和期望。它们通常包括：\n\n性能期望：函数应该在多长时间内完成\n资源使用：函数会消耗多少内存、CPU或网络带宽\n副作用：函数会修改哪些外部状态\n错误处理：在什么情况下函数应该抛出异常，什么情况下应该静默处理\n并发安全：函数是否可以在多线程环境下安全调用\n\n实例分析：AI编程中的隐式契约陷阱案例一：文件读取的”合理”超时# AI生成的代码def read_large_file(file_path):    with open(file_path, &#x27;r&#x27;) as f:        return f.read()\n\n问题：这段代码对于小文件工作正常，但对于10GB的大文件，它会耗尽内存并导致程序崩溃。人类开发者会意识到需要分块读取或使用流式处理，但AI只看到了”读取文件”这个明确需求。\n隐式契约：读取操作应该在合理时间内完成，且不会耗尽系统资源。\n案例二：API调用的”礼貌”重试// AI生成的API调用代码async function fetchUserData(userId) &#123;    const response = await fetch(`/api/users/$&#123;userId&#125;`);    return response.json();&#125;\n\n问题：网络请求可能失败，但代码没有重试机制。人类开发者知道网络是不可靠的，通常会添加重试逻辑、超时处理和错误回退。\n隐式契约：网络操作应该具有弹性，能够处理临时故障。\n案例三：缓存更新的”一致性”保证// AI生成的缓存更新代码public void updateUserCache(User user) &#123;    cache.put(user.getId(), user);    database.update(user);&#125;\n\n问题：如果数据库更新失败，缓存中已经存储了不一致的数据。人类开发者会使用事务或两阶段提交来保证一致性。\n隐式契约：数据更新操作应该保持系统状态的一致性。\n隐式契约的根源：为什么它们如此普遍？1. 历史遗留与约定俗成许多隐式契约源于历史原因。比如，Unix命令行工具遵循”安静成功，详细失败”的原则——这个原则从未在man page中明确写出，但所有有经验的开发者都知道。\n2. 性能与简洁性的权衡明确写出所有契约会使代码变得冗长。例如，每个函数都加上性能保证注释是不现实的：\n# 如果每个函数都这样写...def process_data(data):    &quot;&quot;&quot;    处理数据。        性能契约：    - 时间复杂度：O(n log n)    - 空间复杂度：O(n)    - 最大输入大小：10,000条记录    - 预期执行时间：&lt; 2秒（在标准硬件上）        副作用契约：    - 不会修改输入数据    - 会写入日志文件    - 可能向监控系统发送指标        错误契约：    - 输入为空时返回空列表    - 数据格式错误时抛出ValueError    - 内存不足时抛出MemoryError    &quot;&quot;&quot;    # 实际实现...\n\n3. 领域知识的缺失AI缺乏特定领域的专业知识。医疗软件、金融系统、航空航天控制等领域都有大量的领域特定隐式契约，这些知识通常通过多年经验积累。\n4. 上下文依赖许多契约依赖于具体的使用场景。同一个函数在批处理系统和实时系统中可能有完全不同的性能期望。\n隐式契约带来的具体问题1. 调试困难当违反隐式契约时，错误信息往往不明确。程序可能只是”运行缓慢”或”偶尔崩溃”，而不是抛出清晰的异常。\n2. 集成问题不同团队或不同系统对同一概念可能有不同的隐式契约，导致集成时出现微妙的不兼容。\n3. 技术债务积累随着时间的推移，未被文档化的隐式契约会变成”部落知识”——只有少数老员工知道，新员工和AI助手都会反复踩坑。\n4. 阻碍自动化隐式契约是自动化测试、静态分析和AI代码生成的重大障碍。如果规则不明确，机器就无法可靠地验证或生成代码。\n解决方案：让隐式契约显式化1. 契约优先设计（Contract-First Design）在编写实现之前，先明确写出函数的契约。这可以通过多种形式实现：\nfrom typing import Protocolfrom dataclasses import dataclass@dataclassclass PerformanceContract:    max_time_ms: int    max_memory_mb: int    thread_safe: boolclass DataProcessor(Protocol):    performance: PerformanceContract        def process(self, data: list) -&gt; list:        &quot;&quot;&quot;        契约：        1. 不会修改输入数据        2. 时间复杂度 O(n log n)        3. 空输入返回空列表        &quot;&quot;&quot;        ...\n\n2. 使用契约编程框架利用现有的契约编程工具，如Python的icontract、Java的Contracts for Java或Eiffel语言内置的契约支持：\nimport icontract@icontract.require(lambda x: x &gt; 0, &quot;输入必须为正数&quot;)@icontract.ensure(lambda result: result &gt; 0, &quot;结果必须为正数&quot;)@icontract.snapshot(lambda x: x, &quot;保存原始值&quot;)def calculate_square_root(x: float) -&gt; float:    # 实现必须满足前后条件    return x ** 0.5\n\n3. 增强的API文档在文档中明确列出所有隐式契约。可以使用标准化的模板：\n## 性能特征- **时间复杂度**：O(n)- **空间复杂度**：O(1)- **线程安全**：是## 副作用- 会修改全局配置- 会写入日志文件## 错误处理- 输入无效时抛出`ValueError`- 资源不足时抛出`RuntimeError`\n\n4. 运行时契约检查在开发和测试环境中启用契约检查，在生产环境中关闭以提高性能：\nclass ContractAwareProcessor:    def __init__(self, debug=False):        self.debug = debug        def process(self, data):        if self.debug:            self._check_preconditions(data)                result = self._actual_process(data)                if self.debug:            self._check_postconditions(data, result)                return result\n\n5. AI友好的代码注解为AI助手提供专门的注解，帮助它们理解隐式契约：\n# @ai-contract: 这个函数用于处理用户输入，对性能敏感# @ai-expectation: 应该在100ms内完成# @ai-side-effect: 会更新数据库中的用户状态# @ai-error-case: 网络超时时重试3次def handle_user_request(request):    # 实现...\n\n面向未来的思考1. 契约作为一等公民未来的编程语言可能会将契约作为语言的一等公民，就像类型系统一样。编译器可以静态检查契约，IDE可以提供更好的支持。\n2. AI可理解的契约语言我们需要开发一种既对人类友好，又对AI可解析的契约描述语言。这种语言应该能够表达复杂的约束和期望。\n3. 契约学习与推理AI系统可以通过分析大量代码库，自动学习和推断常见的隐式契约，并建议将它们显式化。\n4. 契约驱动的代码生成未来的AI代码生成器可以以契约为输入，生成满足所有约束的代码实现。\n实践建议给开发者的建议：\n识别关键契约：在代码审查时，特别关注那些可能包含隐式契约的函数\n逐步显式化：不要试图一次性文档化所有契约，从最关键的开始\n建立契约文化：在团队中推广契约优先的思维方式\n利用工具：使用静态分析工具检测可能的契约违反\n\n给AI提示工程师的建议：\n明确表达期望：在提示中不仅说明”做什么”，还要说明”在什么约束下做”\n提供上下文：告诉AI代码将运行在什么环境中，有什么性能要求\n要求契约注释：让AI生成的代码包含明确的契约注释\n测试边界条件：特别测试那些可能违反隐式契约的边缘情况\n\n结语：从隐式到显式的进化隐式契约问题是AI编程成熟过程中的必经阶段。正如软件工程从”写代码”进化到”设计系统”，从”能运行”进化到”可维护”，我们现在正经历从”隐式理解”到”显式表达”的进化。\n解决隐式契约问题不仅是让AI更好地编程，更是让所有软件更加可靠、可维护、可理解。在这个过程中，我们不仅教会了AI如何编程，也教会了自己如何更好地表达意图、管理复杂性和构建健壮的系统。\n最终，显式的契约会成为连接人类意图、机器理解和代码实现的重要桥梁。当这座桥梁建成时，AI编程才能真正从”辅助工具”进化为”可靠伙伴”。\n\n思考题：在你的项目中，有哪些重要的隐式契约？如果让AI来维护你的代码，哪些契约最需要被显式化？\n延伸阅读：\n\n《设计模式：可复用面向对象软件的基础》- 模式本身就是一种高级契约\n《代码大全》- 关于软件构建的全面指南\n《重构：改善既有代码的设计》- 如何安全地修改代码而不违反契约\n\n","categories":["技术思考"],"tags":["AI编程","软件开发","代码质量"]},{"title":"How to Earn Your First Bucket of Gold with AI: 7 Practical Paths for Ordinary People","url":"/2026/03/02/ai-first-bucket-gold-en/","content":"How to Earn Your First Bucket of Gold with AI: 7 Practical Paths for Ordinary PeopleAnother person in my feed is posting income screenshots. The title is always “Making $5K a Month with ChatGPT” or “AI Side Hustle Gave Me Financial Freedom.”\nClick in, and it’s either selling courses or recruiting for MLM. People who actually make money with AI rarely advertise it loudly.\nBut that doesn’t mean opportunities don’t exist. It’s just that real opportunities look very different from what you see in marketing posts.\nLet’s Be Clear: AI Isn’t a Money PrinterLast year, a designer friend asked me: can you make money with Midjourney?\nI said yes, but not the way you think.\nHis idea was simple: generate hundreds of images daily, upload to stock platforms, earn passive income. Two months later, he told me he hadn’t sold a single image.\nSimple reason: too many AI-generated images, insanely fierce competition. Plus, buyers can tell which are AI-generated, and they’d rather pay for work with human touch.\nThat’s the first realization: AI is a tool, not a replacement. It can boost your efficiency, but can’t replace your value.\nContent Creation: Easiest to Start, Also Easiest to FailI know a WeChat blogger who uses ChatGPT to write tech articles. At first, efficiency was indeed high, could produce five or six articles a day. But two months later, followers were dropping fast.\nHe later reviewed and found the problem: AI-generated content flows smoothly but lacks personal viewpoints. After reading three articles, readers can feel “this account is AI-written.”\nHow do successful people do it?\nCase: Xiao Wang’s Tech Blog\nXiao Wang is a backend engineer. His approach:\n\nFirst thinks through solution to a technical problem himself\nUses AI to help organize into article structure\nKey parts (personal insights, pitfall experiences) written by himself\nUses AI to polish language\n\nHis blog now earns about $1,200 monthly, mainly from ads and paid columns.\nKey point: AI handles execution, human handles thinking.\nBut honestly, content creation is getting increasingly competitive. Without depth in a professional field, hard to stand out.\nAI Tool Development: Lower Barrier, But More Intense CompetitionAfter GPT Store launched, many people rushed in to make GPTs. I tried too, made a “Paper Polishing Assistant.”\nSpent two days tuning prompts, testing effects, writing documentation. First week after launch, indeed dozens of people used it. I was excited, thought I’d found a money path.\nThen second week, similar GPTs appeared by the dozens. By third week, people were making better ones, and for free.\nNow this GPT is basically unused.\nWhat’s the lesson? Low-code tools lowered barriers, but also means your moat almost doesn’t exist.\nHowever, some people still succeed.\nCase: Lao Li’s Industry Tool\nLao Li works in construction. He found many engineers need to quickly generate technical specifications, but generic tools on the market don’t work well.\nHe made a specification generator specifically for construction using Claude. Function is simple, but particularly fits industry needs.\nNow he charges monthly, stably earning $400-700 per month. Not many users, but very targeted, low churn rate.\nInsight: Focus on niche verticals, solve real pain points.\nConsulting Services: Knowing AI is a Scarce SkillThe most profitable person I’ve seen does “AI application consulting” for SMEs.\nSounds fancy, but the work is down-to-earth: teaching bosses how to use ChatGPT to improve work efficiency, helping them build simple AI workflows.\nNot cheap, $500+ per consultation. But clients are happy to pay because they can see real results.\nFor example, what he did for a law firm:\n\nUse AI to organize case precedents, saving lawyers 80% of research time\nBuild case information extraction system, auto-generate preliminary analysis reports\nTeach team to use AI to assist in drafting legal documents\n\nThese things aren’t technically complex, but the law firm people don’t know how. He can translate technology into actual value, that’s his core competitiveness.\nKey point: Don’t try to teach people AI technology, help them solve actual problems.\nData Services: Dirty Work, But Actually Makes MoneyData annotation—many people look down on it. Think it’s low-end, repetitive, no technical content.\nBut a friend with a data annotation studio made over $60K last year.\nHis model:\n\nDoesn’t do specific work himself, but organizes manpower\nTakes projects from big platforms, then subcontracts to freelancers\nUses AI to assist quality checks, improving efficiency\n\nSounds simple, but execution requires management skills. You need to recruit, train, quality check, interface with clients.\nThis isn’t a relaxed side hustle, more like entrepreneurship. But relatively low barrier, can stably make money.\nAnother direction is synthetic data.\nMany AI companies now need training data, but real data is hard to obtain. Some people specifically use AI to generate training data, then sell to these companies.\nTechnical barrier isn’t high, but requires understanding client needs. If you have domain knowledge (like healthcare, legal), this is a good direction.\nE-commerce AI Tools: Don’t Build Platform, Provide ServiceMany people want to do “AI e-commerce tools,” make a SaaS product, charge monthly.\nThis path is too hard. You need to develop product, acquire users, continuously operate. Individuals simply can’t handle it.\nBut flip the thinking: don’t make product, provide service.\nCase: A Qiang’s E-commerce Copy Service\nA Qiang used to be in e-commerce operations. His current business: help Taobao shop owners batch-generate optimized product titles and descriptions.\nNot simply using AI to generate, but:\n\nAnalyze shop data, find products with low conversion rates\nCombine industry hot words and search trends, use AI to generate multiple versions\nA&#x2F;B test, find optimal version\nContinuously optimize\n\nHe charges per project, $300-700 per shop. Can do five or six projects a month.\nKey is he’s not selling tools, he’s selling results. Clients want “improved conversion rates,” not “an AI tool.”\nAI Education: Teach Others AI, But Don’t Sell AnxietyAI education market is big, but most is cutting leeks.\nWhat does valuable AI education look like?\nI’ve seen a good example: AI application course for designers.\nThis course doesn’t teach you how to use ChatGPT, but how to integrate AI into design workflow:\n\nUse AI for brainstorming and concept generation\nUse Midjourney for rapid prototyping\nUse AI to assist user research\nIntegrate AI tools into actual projects\n\nCourse costs $300, not cheap. But people who took it say it’s worth it, because they can actually use it.\nCompared to those “learn AI in 3 days, make $10K a month” courses, this is real value.\nIf you have professional accumulation in some field, consider doing this. But remember: don’t sell anxiety, provide value.\nInvestment and Incubation: Hardest, Also Highest ReturnThis path isn’t for ordinary people. Requires capital, resources, vision.\nBut still worth mentioning, because some people do have this capability.\nIf you:\n\nHave some capital (at least tens of thousands in spare cash)\nHave industry connections and resources\nCan identify good projects\n\nConsider early-stage investing in AI projects, or incubate yourself.\nBut this isn’t a side hustle, it’s entrepreneurship. Risk is high, failure rate is also high.\nA friend who does early-stage investing invested in 5 AI projects last year, 4 died, 1 still struggling. He says that’s normal.\nSo don’t be fooled by “investing in AI projects get rich overnight” stories. The real investment world is far crueler than you imagine.\nLet’s Talk StraightBy now, you might have noticed: none of these paths are “get rich quick” methods.\nThat’s the truth. AI isn’t magic for passive income, it’s a tool that lets you do more.\nThose $10K a month cases? Might exist, but they either have deep professional accumulation, strong execution, or got really lucky.\nWhat can ordinary people do?\n\nStart as side hustle: Don’t quit your job to start a business, test waters with spare time first\nFocus on one direction: Don’t try to do everything, pick one and go deep\nProvide real value: Think about what problem you can help others solve\nContinuously iterate: First time definitely won’t be good, key is constant improvement\n\nAnd most importantly: stay skeptical.\nWhen you see “AI money-making” ads, first think: if it’s really that profitable, why would they tell you?\nFinallyAI has indeed created new money-making opportunities, but these opportunities belong to those who can provide value.\nIf you’re just looking for a “passive income” method, this article might disappoint you.\nBut if you’re willing to invest time, accumulate experience, solve real problems, AI can become your good helper.\nYour first bucket of gold won’t fall from the sky, but it might be closer than you think.\nThe prerequisite is, you have to actually do it.\n\nI’m also trying to use AI to improve work efficiency. Some successes, some failures.\nThis article isn’t a guide, it’s observations and thoughts.\nIf you have other real AI money-making experiences, welcome to share.\n","categories":["English"],"tags":["AI Monetization","Side Hustle","Entrepreneurship","Tech Monetization"]},{"title":"如何用AI赚取第一桶金：普通人切实可行的7个路径","url":"/2026/03/02/ai-first-bucket-gold/","content":"如何用AI赚取第一桶金：普通人切实可行的7个路径朋友圈又有人在晒收入截图了。标题永远是”用ChatGPT月入五万”或者”AI副业让我实现财富自由”。\n点进去一看，要么是卖课的，要么是拉下线的。真正靠AI赚到钱的人，很少会大张旗鼓地宣传。\n但这不代表机会不存在。只是，真实的机会和你在营销号看到的很不一样。\n先说清楚：AI不是印钞机去年有个做设计的朋友问我：用Midjourney能不能赚钱？\n我说能，但不是你想的那种赚法。\n他的想法很简单：每天生成几百张图，挂到图库平台上，躺着收钱。结果两个月后，他告诉我一张图都没卖出去。\n原因很简单：AI生成的图片太多了，竞争激烈得离谱。而且买图的人看得出来哪些是AI生成的，他们更愿意为有人味的作品付费。\n这就是第一个认知：AI是工具，不是替代品。它能提升你的效率，但不能替代你的价值。\n内容创作：最容易入门，也最容易失败我认识一个公众号博主，用ChatGPT写科技文章。一开始效率确实高，一天能产出五六篇。但两个月后，掉粉掉得厉害。\n他后来复盘，发现问题在于：AI生成的内容虽然流畅，但缺乏个人观点。读者看三篇就能感觉出来，”这号是AI写的”。\n真正做得好的人是怎么做的？\n案例：小王的技术博客\n小王是个后端工程师。他的做法是：\n\n自己先思考一个技术问题的解决思路\n用AI帮他整理成文章结构\n关键部分（个人见解、踩坑经验）自己写\n用AI润色语言\n\n他的博客现在月收入大概8000块，主要来自广告和付费专栏。\n关键点：AI负责执行，人负责思考。\n但说实话，内容创作这条路越来越卷了。如果你没有专业领域的深度，很难脱颖而出。\nAI工具开发：门槛降低了，但竞争更激烈了GPT Store上线后，很多人涌进去做GPTs。我也试过，做了个”论文润色助手”。\n花了两天时间调试prompt，测试效果，写说明文档。上线后第一周，确实有几十个人用。我很兴奋，觉得找到了赚钱路子。\n然后第二周，类似的GPT就出现了十几个。到第三周，已经有人做得比我好，还免费。\n现在这个GPT基本没人用了。\n教训是什么？低代码工具降低了门槛，但也意味着你的护城河几乎不存在。\n不过，还是有人能做成的。\n案例：老李的行业工具\n老李在建筑行业工作。他发现很多工程师需要快速生成技术说明书，但市面上的通用工具都不好用。\n他用Claude做了一个专门针对建筑行业的说明书生成工具。功能很简单，但特别贴合行业需求。\n现在他按月收费，每个月能稳定赚3000-5000。用户不多，但很精准，流失率低。\n启发：专注细分领域，解决真实痛点。\n咨询服务：会用AI是一种稀缺能力我见过最赚钱的一个人，是给中小企业做”AI应用咨询”的。\n听起来很高大上，实际上做的事情很接地气：教老板们怎么用ChatGPT提升工作效率，帮他们搭建简单的AI工作流。\n收费不便宜，一次咨询3000起。但客户很愿意付钱，因为确实能看到效果。\n比如他帮一家律所做的事情：\n\n用AI整理判例，节省律师80%的查资料时间\n搭建案件信息提取系统，自动生成初步分析报告\n教团队用AI辅助撰写法律文书\n\n这些东西技术上都不复杂，但律所的人不会。他能把技术转化成实际价值，这就是他的核心竞争力。\n关键点：别想着教人AI技术，而是帮人解决实际问题。\n数据服务：脏活累活，但确实赚钱数据标注这个活，很多人看不上。觉得低端、重复、没技术含量。\n但有个做数据标注工作室的朋友，去年赚了四十多万。\n他的模式是这样的：\n\n自己不干具体活，而是组织人力\n接大平台的项目，然后分包给兼职人员\n用AI辅助做质量检查，提高效率\n\n听起来简单，但执行起来需要管理能力。你得招人、培训、质检、对接客户。\n这不是轻松的副业，更像是创业。但门槛相对低，能稳定赚钱。\n另一个方向是合成数据。\n现在很多AI公司需要训练数据，但真实数据不好获取。有人专门用AI生成训练数据，然后卖给这些公司。\n技术门槛不高，但需要理解客户的需求。如果你有特定领域的知识（比如医疗、法律），这是个不错的方向。\n电商AI工具：别做平台，做服务很多人想做”AI电商工具”，做个SaaS产品，收月费。\n这条路太难了。你要开发产品、获取用户、持续运营。个人根本搞不定。\n但换个思路：不做产品，做服务。\n案例：阿强的电商文案服务\n阿强原来是电商运营。他现在的业务是：帮淘宝店家批量生成优化后的商品标题和描述。\n不是简单地用AI生成，而是：\n\n分析店铺的数据，找到转化率低的商品\n结合行业热词和搜索趋势，用AI生成多个版本\nA&#x2F;B测试，找出最优版本\n持续优化\n\n他按项目收费，一个店铺一次收2000-5000。一个月能做五六个项目。\n关键是他不是在卖工具，而是在卖结果。客户要的是”提升转化率”，不是”一个AI工具”。\nAI教育：教别人用AI，但别卖焦虑AI教育市场很大，但大部分是割韭菜的。\n真正有价值的AI教育是什么样的？\n我见过一个做得好的例子：针对设计师的AI应用课程。\n这个课程不是教你怎么用ChatGPT，而是教你怎么把AI融入设计工作流：\n\n用AI做头脑风暴和概念生成\n用Midjourney做快速原型\n用AI辅助做用户研究\n整合AI工具到实际项目中\n\n课程1980元，不便宜。但上过课的人都说值，因为确实能用上。\n对比那些”3天学会用AI月入十万”的课程，这才是真正的价值。\n如果你在某个领域有专业积累，可以考虑做这个。但记住：别卖焦虑，提供价值。\n投资与孵化：这是最难的，也是回报最高的这条路不是给普通人准备的。需要资金、资源、眼光。\n但还是要提一下，因为有些人确实有这个能力。\n如果你：\n\n有一定资金（至少几十万闲钱）\n有行业人脉和资源\n能识别好的项目\n\n可以考虑早期投资AI相关项目，或者自己孵化。\n但这不是副业，是创业。风险很高，失败率也很高。\n我认识一个做早期投资的朋友，去年投了5个AI项目，4个死了，1个还在挣扎。他说这就是常态。\n所以别被”投资AI项目一夜暴富”的故事迷惑。真实的投资世界，远比你想象的残酷。\n说点实话写到这里，你可能发现了：这些路径都不是”快速赚钱”的方法。\n事实就是这样。AI不是让你躺着赚钱的魔法，而是让你能做更多事的工具。\n那些月入十万的案例？可能存在，但他们要么有深厚的专业积累，要么有很强的执行力，要么运气特别好。\n普通人能做什么？\n\n从副业开始：别辞职创业，先用业余时间试水\n聚焦一个方向：别什么都想做，选一个深入下去\n提供真实价值：想想你能帮别人解决什么问题\n持续迭代：第一次肯定做不好，关键是不断改进\n\n还有最重要的一点：保持怀疑。\n看到”AI赚钱”的广告，先想想：如果真那么好赚，他为什么要告诉你？\n最后AI确实创造了新的赚钱机会，但这些机会属于那些能提供价值的人。\n如果你只是想找个”躺着赚钱”的方法，这篇文章可能让你失望了。\n但如果你愿意投入时间、积累经验、解决真实问题，AI能成为你的好帮手。\n第一桶金不会从天而降，但它可能比你想象的更近。\n前提是，你得动手去做。\n\n我自己也在尝试用AI提升工作效率。有成功，有失败。\n这篇文章不是指南，是观察和思考。\n如果你有其他真实的AI赚钱经验，欢迎分享。\n","categories":["技术思考"],"tags":["AI赚钱","副业","创业","技术变现"]},{"title":"AI 时代的新公司模式","url":"/2026/02/28/ai-company-model/","content":"一个人干翻一个团队最近一两年，我注意到一个越来越明显的趋势：很多让人惊艳的产品，背后只有一两个人。\n一个独立开发者，用 AI 辅助写代码、做设计、写文案、搞运营，几周时间就能上线一个完整的产品。放在三年前，同样的事情需要一个五到十人的小团队干几个月。\n这不是个例。你去看 Product Hunt 上的热门产品，越来越多标注着”Solo Founder”。你去看 GitHub 上的热门项目，很多 contributor 只有一两个人，但代码质量和功能完整度不输大团队的作品。\n发生了什么？\n答案很简单：AI 改变了生产力的方程式。\n旧模式：人多力量大传统的公司模式建立在一个基本假设上：做更多的事，需要更多的人。\n你要开发一个产品，需要产品经理、设计师、前端工程师、后端工程师、测试工程师、运维工程师。你要推广这个产品，需要市场、运营、客服。你要管理这些人，需要项目经理、HR、行政。\n人一多，沟通成本就上来了。五个人的团队，沟通线路是 10 条。十个人就变成 45 条。五十个人是 1225 条。这就是为什么大公司效率低——不是人不行，是沟通的复杂度随人数指数增长。\n于是你需要层级、流程、会议、文档、审批。这些东西本身不创造价值，但没有它们，大组织就会失控。管理成本成了一个巨大的隐性税。\n旧模式的本质是：用管理成本换取规模效应。\n这在工业时代是合理的。流水线需要工人，工人需要管理。但在信息时代，尤其是 AI 时代，这个等式开始松动了。\nAI 改变了什么AI 做了一件事：把很多需要”一个人”的工作，变成了需要”一个人的一个小时”的工作。\n写一篇市场文案，以前需要一个文案策划花一天。现在你跟 AI 说清楚需求，十分钟出初稿，再花二十分钟修改润色。\n做一个产品原型，以前需要设计师画几天。现在你用 AI 生成界面，自己微调，半天搞定。\n写一个后端接口，以前需要工程师写半天加调试。现在 AI 写完你 review 一下，一个小时连测试都跑完了。\n单个环节的效率提升不是重点。重点是：当每个环节都提速 5-10 倍，一个人就能覆盖原来好几个人的工作范围。\n这不是说 AI 替代了人。而是 AI 让一个人的能力边界大幅扩展了。一个懂产品的工程师，加上 AI，可以同时扮演产品经理、设计师、前端、后端、测试、文案的角色。不是每个角色都做到 100 分，但做到 70-80 分已经足够上线一个产品了。\n\n新模式一：超级个体第一种新模式是超级个体——一个人就是一家公司。\n这不是新概念，自由职业者一直存在。但 AI 时代的超级个体和以前不一样。以前的自由职业者通常只能做一件事——你是设计师就做设计，你是程序员就写代码。现在一个人可以做全栈：从想法到产品到上线到运营，全链路自己搞定。\n我见过一个独立开发者，一个人做了一个 SaaS 产品，月收入几万美元。他的”团队”是：自己 + ChatGPT + Cursor + Midjourney + 几个自动化工具。客服用 AI 聊天机器人处理，财务用 Stripe 自动化，部署用 Vercel 一键搞定。\n这种模式的优势很明显：\n\n零沟通成本。 所有决策都在一个脑子里完成，不需要开会、对齐、等审批。\n极致灵活。 今天想加个功能，下午就上线了。想换个方向，明天就换。\n利润率极高。 没有人力成本，收入几乎等于利润。\n\n当然也有局限：一个人的精力有限，做不了太大的事。但”太大”的标准在不断提高——以前一个人做不了的事，现在可以了。\n新模式二：AI-Native 小团队第二种模式是 AI-Native 小团队——三到十个人，做出以前需要五十人才能做的产品。\n这种团队的特点是：每个人都是全栈型人才，AI 是每个人的”副驾驶”。\n传统团队的分工是按职能划分的：产品组、设计组、开发组、测试组。AI-Native 团队的分工是按业务模块划分的：每个人负责一个完整的模块，从需求到上线全包。AI 帮他补齐不擅长的环节。\n我观察到的一些特征：\n极少的会议。 因为每个人都能独立做决策，不需要频繁对齐。异步沟通为主，文档驱动。\n没有专职管理者。 团队小到不需要”管理”。大家都是 maker，没有人只做 manager。\nAI 工具深度集成到工作流。 不是偶尔用一下 ChatGPT，而是 AI 渗透到每个环节——写代码用 Cursor，做设计用 AI 生成，写文档用 AI 辅助，做数据分析用 AI 处理。\n招人标准变了。 不再看你会多少种编程语言或者用过多少种框架。看的是：你能不能独立把一件事从头到尾做完？你能不能高效地使用 AI 工具？你有没有产品感觉？\n这种模式的代表是很多新兴的 AI 创业公司。你会发现，做出惊艳产品的公司，团队往往小得出奇。\n新模式三：动态网络第三种模式更激进：公司不再是一个固定的组织，而是一个动态的协作网络。\n核心团队可能只有两三个人，负责产品方向和核心技术。其他工作通过外包、合同工、AI Agent 的组合来完成。需要做一个营销活动？找一个自由职业的营销专家合作两周。需要做一个复杂的数据分析？让 AI Agent 跑一遍，人工审核结果。\n这种模式的本质是：把固定成本变成可变成本。\n传统公司养一个全职员工，不管他这个月忙不忙，工资照发。动态网络模式下，你只在需要的时候付费。AI 进一步降低了这种模式的摩擦——以前找外包、对接、沟通、验收的成本很高，现在 AI 可以承担很多中间环节。\n对传统大公司的冲击这些新模式对传统大公司意味着什么？\n首先，竞争对手变多了。 以前只有大公司才能做的产品，现在小团队也能做。而且小团队更快、更灵活、更敢冒险。大公司花一年做的功能，小团队可能两个月就做出来了。\n其次，人才流失加速。 最优秀的人才发现，自己加上 AI 就能创造巨大价值，为什么要待在大公司里开会、写周报、等审批？超级个体和小团队模式对顶尖人才的吸引力越来越大。\n第三，组织臃肿的代价更高了。 以前大公司的效率低一点没关系，因为竞争对手也差不多。现在不一样了——你的竞争对手可能是一个三人团队，他们的决策速度是你的十倍，迭代速度是你的五倍。\n这不是说大公司会消失。大公司有自己的优势：品牌、渠道、数据、资金、合规能力。但大公司需要变革——要么变小，要么变快，要么做小公司做不了的事。\n\n新的竞争壁垒在新模式下，竞争壁垒也在变化。\n以前的壁垒： 人多、钱多、技术积累深。你有一千个工程师，别人追不上你。\n现在的壁垒：\n\n数据飞轮。 谁的产品有更多用户、产生更多数据、训练出更好的模型，谁就有壁垒。\nAI 使用效率。 同样用 AI，有人能把效率提升 10 倍，有人只能提升 2 倍。差距在于对 AI 能力边界的理解和工作流的设计。\n速度。 在 AI 时代，速度就是壁垒。你比别人快两周上线，就能先抢到用户和反馈数据。\n品味和判断力。 当 AI 把执行成本降到极低，决定胜负的是”做什么”而不是”怎么做”。产品品味、战略判断力变得比以往任何时候都重要。\n信任和品牌。 AI 能帮你做产品，但不能帮你建立信任。用户选择你而不是竞争对手，越来越取决于品牌和口碑。\n\n未来的公司长什么样如果把这些趋势推演下去，未来的公司可能长这样：\n大多数公司会变小。 不是因为做的事变少了，而是因为同样的事需要的人变少了。一个 50 人的公司可能缩到 15 人，产出不变甚至更高。\n组织结构会更扁平。 中间管理层会被压缩。当每个人都能独立做决策、AI 能处理大量协调工作时，层级就没有存在的必要了。\n全职员工和外部协作者的边界会模糊。 核心团队 + 动态外部网络会成为常态。\nAI 会成为”员工”。 不是比喻，是字面意思。公司会有 AI Agent 负责客服、数据分析、内容生产、代码审查。这些 Agent 有自己的”工位”（运行环境）、”权限”（API access）、”考核”（质量监控）。\n创业门槛会大幅降低。 一个有想法的人，不需要融资、不需要招人，就能做出一个有竞争力的产品。这意味着创业的数量会爆发，但成功的标准也会提高——因为竞争更激烈了。\n写在最后每次技术革命都会重塑组织形式。\n工业革命催生了工厂和流水线。信息革命催生了互联网公司和远程办公。AI 革命正在催生新的公司模式——更小、更快、更灵活，AI 深度嵌入每个环节。\n这对个人来说，既是机会也是挑战。机会在于：你不再需要一个大团队才能做大事。挑战在于：如果你不学会和 AI 协作，你的竞争力会快速下降。\n我的判断是：未来五年，”会用 AI”会像”会用电脑”一样，从加分项变成基本功。 不是你要不要学的问题，而是你什么时候开始学的问题。\n而那些最早拥抱新模式的人和公司，会拿到最大的红利。\n就像每次技术革命一样——先知先觉者吃肉，后知后觉者喝汤，不知不觉者买单。\n","categories":["技术"],"tags":["AI","技术思考","商业"]},{"title":"The Infrastructure of the AI Era","url":"/2026/02/28/ai-infrastructure-en/","content":"The Road BuildersThere’s a saying I grew up hearing: if you want to get rich, build roads first.\nThis applies to technology just as well. Every technological revolution is preceded by massive infrastructure buildout. The steam age built railways. The electrical age strung power grids. The internet age laid fiber optic cables and erected data centers. Infrastructure isn’t glamorous, but without it, even the best technology can’t run.\nNow it’s AI’s turn.\nSince GPT came along, everyone’s been talking about how smart the models are and what they can do. But few people seriously discuss: what roads do we need to build to make AI actually work?\nThat’s what this article is about — what AI-era infrastructure actually looks like.\nCompute: The Most Visible LayerWhen people think of AI infrastructure, GPUs come to mind first. And yes, compute is the most visible and expensive layer. NVIDIA’s market cap tells the story.\nBut the compute story is far more complex than “buy more cards.”\nTraining a large model requires thousands of GPUs working in concert for months. Behind this lies distributed computing, high-speed interconnects (InfiniBand&#x2F;NVLink), large-scale cluster scheduling, fault recovery… each one a hardcore engineering challenge. How many failures OpenAI’s cluster experienced during GPT-4 training, how many checkpoint recoveries they performed — outsiders can barely imagine.\nThe inference side presents entirely different challenges. Training is a one-time cost (albeit expensive), but inference is continuous — every user’s every conversation consumes compute. When your product has hundreds of millions of users, inference cost is the real heavyweight. That’s why everyone’s racing on inference optimization: quantization, distillation, speculative decoding, KV cache optimization…\nBut compute is just the tip of the iceberg. Just as the internet era needed more than servers — it needed CDNs, load balancers, databases — the AI era needs far more than GPUs.\nData: Scarcer Than ComputeThere’s a pattern that’s been validated repeatedly: data quality determines a model’s ceiling; compute only determines how fast you approach it.\nPublic text on the internet has been scraped nearly dry. Every model company is struggling for high-quality data. You see seemingly absurd news — one company buying an entire publisher’s catalog, another hiring tens of thousands of annotators, another using its own model to generate synthetic training data.\nData infrastructure has several layers:\nCollection and cleaning. Raw data is dirty, duplicated, biased. Turning it into usable training data requires an entire pipeline: deduplication, filtering, anonymization, formatting. This work isn’t glamorous, but it determines a model’s character.\nAnnotation and alignment. RLHF (Reinforcement Learning from Human Feedback) requires massive amounts of high-quality human preference data. Annotator quality directly affects a model’s “values.” This is a labor-intensive step and the most easily underestimated one.\nData flywheels. The truly powerful companies don’t just solve data once — they build data flywheels. Users generate data through the product, data improves the model, the model improves the product, the product attracts more users. ChatGPT’s data flywheel is already spinning, and this is the hardest moat for newcomers to cross.\nModel Serving: From Lab to ProductionTraining a good model is just the beginning. Turning it into a stable, efficient, scalable service is another massive engineering effort.\n\nSeveral key challenges here:\nInference engines. vLLM, TensorRT-LLM, SGLang… these frameworks make the same GPUs serve more requests. Continuous batching, PagedAttention, speculative decoding — each optimization can multiply throughput several times over.\nModel routing. Not every request needs the largest model. Answering a simple greeting with GPT-4 is wasteful. Smart routing systems dispatch requests to appropriate models based on complexity — simple ones to small models, complex ones to large models. Saves money and improves speed.\nCaching and precomputation. Many requests are similar. Semantic caching can return answers for similar questions directly, skipping inference. Prompt prefix caching can reuse KV caches, reducing redundant computation.\nObservability. Models aren’t deterministic systems — the same input can produce different outputs. You need to monitor latency, throughput, error rates, and also output quality — hallucinations, harmful content, deviation from expectations. This is far more complex than traditional APM.\nAgent Infrastructure: The Underestimated New FrontierIf large models are the AI era’s “engines,” then Agents are the “vehicles.” And Agents need their own infrastructure to run.\nMemory systems. I discussed this in detail in my previous article. Agents need short-term memory (current conversation), working memory (current task context), and long-term memory (user preferences and historical knowledge). Most Agent memory systems today are primitive — either stuffing everything into the context window or using RAG retrieval. The future demands more elegant memory architectures.\nTool ecosystems. An Agent’s capabilities depend on what tools it can invoke. Browsers, code executors, file systems, API calls… each tool needs standardized interfaces, permission controls, error handling. MCP (Model Context Protocol) is attempting to solve this, but it’s still very early.\nOrchestration frameworks. Complex tasks require multiple Agents collaborating, or a single Agent executing multi-step workflows. LangChain, CrewAI, AutoGen are all working on this, but honestly, current orchestration frameworks are still rough. The real challenge isn’t “how to chain things together” but “what happens when things go wrong” — retry, rollback, human intervention, partial recovery. Problems already solved in traditional workflow engines need to be re-solved in the Agent domain.\nSandboxing and security. Agents can execute code, access file systems, operate browsers — meaning they have the power to cause damage. You need sandboxes to limit their capabilities, audit logs to track their actions, human approval mechanisms to intercept high-risk operations.\nEvaluation: AI’s Quality Control SystemTraditional software has unit tests, integration tests, stress tests. Evaluating AI systems is much harder because outputs are non-deterministic and the definition of “correct” itself is fuzzy.\nBut without an evaluation system, you’re flying blind.\nBenchmarks. MMLU, HumanEval, GSM8K… these public benchmarks are useful but limited — models might score well on benchmarks while performing poorly in real scenarios.\nDomain evaluation. Every specific application needs its own evaluation set. Building a customer service bot? Evaluate with real customer service conversations. Building a code assistant? Evaluate with real coding tasks. Building high-quality domain evaluation sets is itself an infrastructure effort.\nOnline evaluation. A&#x2F;B testing, user satisfaction, task completion rates… these metrics need continuous collection in production. And you need to distinguish between “the model got better” and “the prompt got better” and “the users changed” — far more complex than traditional A&#x2F;B testing.\nRed teaming. Specifically hunting for model vulnerabilities — can it be tricked into producing harmful content, can safety restrictions be bypassed, will it leak training data. This is an adversarial process requiring dedicated teams and tools.\nDeveloper Perspective: A New Development Paradigm\nAs a developer, what I feel most deeply is: AI is changing the act of “writing code” itself.\nThe old development paradigm: write code → compile → test → deploy. Now there’s an additional dimension: write prompt → call model → evaluate → iterate. This isn’t replacement but addition. Your system has both deterministic code logic and non-deterministic model calls, and they need to work together.\nThis creates new infrastructure needs:\nPrompt management. Prompts are the new era’s “code” — they need version control, A&#x2F;B testing, gradual rollout. But most teams still hardcode prompts, requiring a full deployment to change one.\nModel gateways. Your application might call multiple model providers simultaneously — OpenAI, Anthropic, locally deployed open-source models. You need a unified gateway to manage API keys, load balance, handle degradation, control costs.\nDevelopment tools. AI assistants in IDEs (Copilot, Cursor) are just the beginning. Future development tools will deeply integrate AI — not just code completion, but understanding your entire project, helping with architecture decisions, automatically writing tests, automatically doing code review.\nCost management. AI calls are billed per token, and prices vary enormously — GPT-4 costs dozens of times more than GPT-3.5. You need to monitor AI costs per feature, set budgets, find the balance between quality and cost.\nThe Endgame: AI Like Water and ElectricityBack to the original analogy.\nWhen electricity first appeared, every factory built its own power station. Then came the power grid, and electricity became a public utility — you didn’t need to know how it was generated, just plug in and go.\nThe internet went through a similar process. From self-hosted servers to colocation, to cloud computing, to serverless — abstraction layers kept rising, and developers needed to worry about fewer and fewer low-level details.\nThe endgame of AI infrastructure should be the same.\nDevelopers shouldn’t need to worry about GPU scheduling, model deployment, inference optimization. They should just say “I need a natural language understanding interface” or “I need an image analysis capability,” and the infrastructure layer handles everything automatically.\nWe’re still in the “build your own power station” phase. Every company is setting up their own GPU clusters, training their own models, building their own inference services. This is normal — early stages of new technology are always like this. But the trend is clear: standardization, service-ification, democratization.\nIn the next five years, AI infrastructure will undergo rapid standardization. Just as AWS defined the basic shape of cloud computing, some company will define the basic shape of AI infrastructure. By then, “using AI” will be as natural as “using a database” — you won’t need to be an AI expert to leverage AI capabilities in your product.\nFinal ThoughtsEvery era’s infrastructure is unglamorous. Road builders aren’t as flashy as train riders. Data center builders aren’t as famous as app makers. But without them, trains can’t run and apps can’t load.\nThe AI era is the same. The spotlight falls on models and applications, but what truly determines how far this era goes is the unglamorous infrastructure underneath — data pipelines, inference engines, Agent frameworks, evaluation systems, development tools.\nIf you want to get rich, build roads first. This remains true in the AI era.\nIf you’re a developer, my advice is: don’t just chase model hype. Look at what’s happening in the infrastructure layer. That’s where the more durable opportunities and more solid value lie.\nAfter all, when the tide goes out, what remains is infrastructure.\n","categories":["English"],"tags":["AI","Thoughts","English","Infrastructure"]},{"title":"AI 时代的基建","url":"/2026/02/28/ai-infrastructure/","content":"修路的人小时候听过一句话：要想富，先修路。\n这话放到技术领域同样成立。每一次技术革命的爆发，背后都有一轮大规模的基础设施建设。蒸汽机时代修铁路，电气时代架电网，互联网时代铺光纤建数据中心。基建不性感，但没有它，再好的技术也跑不起来。\n现在轮到 AI 了。\nGPT 出来之后，所有人都在聊模型多聪明、能干什么。但很少有人认真聊过：要让 AI 真正跑起来，我们需要修什么路？\n这篇文章想聊的就是这件事——AI 时代的基建，到底长什么样。\n算力：最显眼的那一层提到 AI 基建，大多数人第一反应是 GPU。没错，算力是最显眼、最烧钱的一层。英伟达的市值说明了一切。\n但算力的故事远不止”买更多卡”这么简单。\n训练一个大模型需要几千张 GPU 协同工作几个月，这背后是分布式计算、高速互联网络（InfiniBand&#x2F;NVLink）、大规模集群调度、故障恢复……每一项都是硬核工程。OpenAI 训练 GPT-4 的集群出过多少次故障、做了多少次 checkpoint 恢复，外面的人很难想象。\n而推理侧的挑战又完全不同。训练是一次性的（虽然很贵），但推理是持续的——每个用户的每次对话都在消耗算力。当你的产品有几亿用户时，推理成本才是真正的大头。所以你看到各家都在卷推理优化：量化、蒸馏、投机解码、KV Cache 优化……\n但算力只是冰山一角。 就像互联网时代光有服务器不够，还需要 CDN、负载均衡、数据库一样，AI 时代光有 GPU 也远远不够。\n数据：比算力更稀缺的资源有一个被反复验证的规律：数据的质量决定了模型的上限，算力只决定你能多快逼近这个上限。\n互联网上的公开文本已经被刮得差不多了。各家模型公司都在为高质量数据发愁。你会看到一些看似荒诞的新闻——某公司花大价钱买下了一整个出版社的版权，某公司雇了几万人做数据标注，某公司用自己的模型生成合成数据再拿来训练。\n数据基建包含几个层面：\n采集和清洗。 原始数据是脏的、重复的、有偏见的。把它变成可用的训练数据，需要一整套 pipeline：去重、过滤、脱敏、格式化。这些工作不光彩，但决定了模型的底色。\n标注和对齐。 RLHF（基于人类反馈的强化学习）需要大量高质量的人类偏好数据。标注员的水平直接影响模型的”三观”。这是一个劳动密集型的环节，也是最容易被低估的环节。\n数据飞轮。 真正厉害的公司不是一次性搞定数据，而是建立了数据飞轮——用户使用产品产生数据，数据反哺模型，模型改善产品，产品吸引更多用户。ChatGPT 的数据飞轮已经转起来了，这是后来者最难追赶的壁垒。\n模型服务：从实验室到生产环境训练出一个好模型只是开始。把它变成一个稳定、高效、可扩展的服务，又是另一个巨大的工程。\n\n这里面有几个关键问题：\n推理引擎。 vLLM、TensorRT-LLM、SGLang……这些推理框架做的事情是：让同样的 GPU 能服务更多的请求。Continuous batching、PagedAttention、Speculative decoding——每一个优化都能带来几倍的吞吐提升。\n模型路由。 不是所有请求都需要最大的模型。一个简单的问候用 GPT-4 回答是浪费。智能路由系统根据请求的复杂度，把它分发到合适的模型——简单的用小模型，复杂的用大模型，既省钱又快。\n缓存和预计算。 很多请求是相似的。语义缓存可以把相似问题的答案直接返回，省掉推理开销。Prompt 前缀缓存可以复用 KV Cache，减少重复计算。\n可观测性。 模型不是确定性系统，同样的输入可能给出不同的输出。你需要监控延迟、吞吐、错误率，还需要监控输出质量——有没有幻觉、有没有有害内容、有没有偏离预期。这比传统的 APM 复杂得多。\nAgent 基础设施：被低估的新战场如果说大模型是 AI 时代的”发动机”，那 Agent 就是”整车”。而 Agent 要跑起来，需要自己的一套基建。\n记忆系统。 我在上一篇文章里详细聊过这个。Agent 需要短期记忆（当前对话）、工作记忆（当前任务上下文）、长期记忆（用户偏好和历史知识）。现在大多数 Agent 的记忆还很原始——要么全塞进 context window，要么用 RAG 检索。未来需要更优雅的记忆架构。\n工具生态。 Agent 的能力取决于它能调用什么工具。浏览器、代码执行器、文件系统、API 调用……每一个工具都需要标准化的接口、权限控制、错误处理。MCP（Model Context Protocol）在尝试解决这个问题，但还很早期。\n编排框架。 复杂任务需要多个 Agent 协作，或者一个 Agent 执行多步骤的工作流。LangChain、CrewAI、AutoGen 都在做这件事，但说实话，现在的编排框架还很粗糙。真正的挑战不是”怎么串起来”，而是”出错了怎么办”——重试、回滚、人工介入、部分恢复，这些在传统工作流引擎里已经解决的问题，在 Agent 领域还需要重新解决。\n沙箱和安全。 Agent 能执行代码、访问文件系统、操作浏览器——这意味着它有能力搞破坏。你需要沙箱来限制它的能力边界，需要审计日志来追踪它做了什么，需要人工审批机制来拦截高风险操作。\n评估：AI 的”质检体系”传统软件有单元测试、集成测试、压力测试。AI 系统的评估要难得多，因为输出是非确定性的，”正确”的定义本身就模糊。\n但没有评估体系，你就是在盲人摸象。\n基准测试。 MMLU、HumanEval、GSM8K……这些公开 benchmark 有用，但也有局限——模型可能在 benchmark 上刷分，在实际场景中拉胯。\n领域评估。 每个具体应用都需要自己的评估集。你做客服机器人，就需要用真实的客服对话来评估；你做代码助手，就需要用真实的代码任务来评估。构建高质量的领域评估集，本身就是一项基建工作。\n在线评估。 A&#x2F;B 测试、用户满意度、任务完成率……这些指标需要在生产环境中持续收集。而且你需要区分”模型变好了”和”prompt 变好了”和”用户变了”——这比传统的 A&#x2F;B 测试复杂得多。\n红队测试。 专门找模型的漏洞——能不能被诱导说出有害内容、能不能被绕过安全限制、会不会泄露训练数据。这是一个攻防对抗的过程，需要专门的团队和工具。\n开发者视角：新时代的开发范式\n作为一个开发者，我感受最深的是：AI 正在改变”写代码”这件事本身。\n以前的开发范式是：写代码 → 编译 → 测试 → 部署。现在多了一个维度：写 prompt → 调模型 → 评估 → 迭代。 这不是替代，而是叠加。你的系统里既有确定性的代码逻辑，也有非确定性的模型调用，两者需要协同工作。\n这带来了新的基建需求：\nPrompt 管理。 Prompt 是新时代的”代码”，需要版本控制、A&#x2F;B 测试、灰度发布。但现在大多数团队还是把 prompt 硬编码在代码里，改一个 prompt 要发一次版。\n模型网关。 你的应用可能同时调用多个模型提供商——OpenAI、Anthropic、本地部署的开源模型。你需要一个统一的网关来管理 API key、做负载均衡、处理降级、控制成本。\n开发工具。 IDE 里的 AI 助手（Copilot、Cursor）只是开始。未来的开发工具会深度集成 AI——不只是补全代码，而是理解你的整个项目、帮你做架构决策、自动写测试、自动做 code review。\n成本管理。 AI 调用是按 token 计费的，而且价格差异巨大——GPT-4 的价格是 GPT-3.5 的几十倍。你需要监控每个功能的 AI 成本，做预算控制，在质量和成本之间找平衡。\n终局：像水电一样的 AI回到最开始的类比。\n电力刚出现的时候，每个工厂都自己建发电站。后来有了电网，电变成了公共服务——你不需要知道电是怎么发的，插上插头就能用。\n互联网也经历了类似的过程。从自建机房到托管，到云计算，到 Serverless——抽象层越来越高，开发者需要关心的底层细节越来越少。\nAI 基建的终局，也应该是这样。\n开发者不需要关心 GPU 调度、模型部署、推理优化。他只需要说”我需要一个能理解自然语言的接口”或者”我需要一个能分析图片的能力”，基建层自动搞定一切。\n我们现在还在”自建发电站”的阶段。各家公司都在自己搭 GPU 集群、自己训模型、自己建推理服务。这很正常——新技术的早期总是这样。但趋势是清晰的：标准化、服务化、平民化。\n未来五年，AI 基建会经历一轮快速的标准化。就像 AWS 定义了云计算的基本形态一样，会有公司定义 AI 基建的基本形态。到那时候，”用 AI”会像”用数据库”一样自然——你不需要成为 AI 专家，就能在你的产品里用上 AI 能力。\n写在最后每个时代的基建都不性感。修铁路的人不如坐火车的人光鲜，建数据中心的人不如做 App 的人出名。但没有他们，火车跑不起来，App 也打不开。\nAI 时代也一样。聚光灯打在模型和应用上，但真正决定这个时代能走多远的，是底下那些不起眼的基建——数据管道、推理引擎、Agent 框架、评估体系、开发工具。\n要想富，先修路。这句话，在 AI 时代依然成立。\n如果你是一个开发者，我的建议是：不要只追模型的热点，也看看基建层在发生什么。那里有更持久的机会，也有更扎实的价值。\n毕竟，潮水退去之后，留下来的是基础设施。\n","categories":["技术"],"tags":["AI","技术思考","基础设施"]},{"title":"One of AI Coding's Stumbling Blocks: The Problem of Implicit Contracts in Programs","url":"/2026/03/02/ai-coding-implicit-contracts-en/","content":"One of AI Coding’s Stumbling Blocks: The Problem of Implicit Contracts in Programs\nIntroduction: When AI Meets “Unspoken” RulesRecently, while experimenting with AI assistants for code generation, I’ve noticed an interesting phenomenon: some code looks perfect—clear logic, correct syntax—but simply doesn’t work as expected. Upon deeper analysis, I found that the root cause often lies not in the code itself, but in those implicit contracts that are never explicitly written down yet are crucial for program execution.\nThese implicit contracts are like the “unwritten rules” of software development—human developers understand them through experience and context, but AI often stumbles when encountering these rules.\nWhat Are Implicit Contracts?Implicit contracts refer to assumptions, conventions, and expectations that are not explicitly stated in code or documentation but are essential for correct program operation. They typically include:\n\nPerformance expectations: How long a function should take to complete\nResource usage: How much memory, CPU, or network bandwidth a function will consume\nSide effects: Which external states a function will modify\nError handling: Under what conditions a function should throw exceptions vs. handle silently\nConcurrency safety: Whether a function can be safely called in a multithreaded environment\n\nCase Studies: Implicit Contract Traps in AI ProgrammingCase 1: The “Reasonable” Timeout for File Reading# AI-generated codedef read_large_file(file_path):    with open(file_path, &#x27;r&#x27;) as f:        return f.read()\n\nProblem: This code works fine for small files, but for a 10GB file, it will exhaust memory and crash the program. Human developers would realize the need for chunked reading or streaming, but AI only sees the explicit requirement to “read a file.”\nImplicit contract: Read operations should complete within a reasonable time and not exhaust system resources.\nCase 2: The “Polite” Retry for API Calls// AI-generated API call codeasync function fetchUserData(userId) &#123;    const response = await fetch(`/api/users/$&#123;userId&#125;`);    return response.json();&#125;\n\nProblem: Network requests can fail, but the code has no retry mechanism. Human developers know networks are unreliable and typically add retry logic, timeout handling, and error fallbacks.\nImplicit contract: Network operations should be resilient and handle temporary failures.\nCase 3: The “Consistency” Guarantee for Cache Updates// AI-generated cache update codepublic void updateUserCache(User user) &#123;    cache.put(user.getId(), user);    database.update(user);&#125;\n\nProblem: If the database update fails, the cache already contains inconsistent data. Human developers would use transactions or two-phase commits to ensure consistency.\nImplicit contract: Data update operations should maintain system state consistency.\nThe Roots of Implicit Contracts: Why Are They So Prevalent?1. Historical Legacy and Established ConventionsMany implicit contracts originate from historical reasons. For example, Unix command-line tools follow the principle of “silent success, verbose failure”—a principle never explicitly stated in man pages but known to all experienced developers.\n2. Trade-offs Between Performance and ConcisenessExplicitly writing all contracts would make code verbose. For instance, adding performance guarantee comments to every function is impractical:\n# If every function were written like this...def process_data(data):    &quot;&quot;&quot;    Process data.        Performance contract:    - Time complexity: O(n log n)    - Space complexity: O(n)    - Maximum input size: 10,000 records    - Expected execution time: &lt; 2 seconds (on standard hardware)        Side effect contract:    - Will not modify input data    - Will write to log file    - May send metrics to monitoring system        Error contract:    - Returns empty list for empty input    - Raises ValueError for malformed data    - Raises MemoryError for insufficient memory    &quot;&quot;&quot;    # Actual implementation...\n\n3. Lack of Domain KnowledgeAI lacks domain-specific expertise. Fields like medical software, financial systems, and aerospace controls have numerous domain-specific implicit contracts typically accumulated through years of experience.\n4. Context DependenceMany contracts depend on specific usage contexts. The same function may have completely different performance expectations in batch processing systems versus real-time systems.\nSpecific Problems Caused by Implicit Contracts1. Difficult DebuggingWhen implicit contracts are violated, error messages are often unclear. Programs may simply “run slowly” or “crash occasionally” rather than throwing clear exceptions.\n2. Integration IssuesDifferent teams or systems may have different implicit contracts for the same concept, leading to subtle incompatibilities during integration.\n3. Accumulation of Technical DebtOver time, undocumented implicit contracts become “tribal knowledge”—known only to a few senior employees, while new hires and AI assistants repeatedly encounter the same pitfalls.\n4. Hindrance to AutomationImplicit contracts are significant obstacles to automated testing, static analysis, and AI code generation. If rules aren’t explicit, machines cannot reliably verify or generate code.\nSolutions: Making Implicit Contracts Explicit1. Contract-First DesignDefine function contracts explicitly before writing implementations. This can be achieved in various forms:\nfrom typing import Protocolfrom dataclasses import dataclass@dataclassclass PerformanceContract:    max_time_ms: int    max_memory_mb: int    thread_safe: boolclass DataProcessor(Protocol):    performance: PerformanceContract        def process(self, data: list) -&gt; list:        &quot;&quot;&quot;        Contract:        1. Will not modify input data        2. Time complexity O(n log n)        3. Returns empty list for empty input        &quot;&quot;&quot;        ...\n\n2. Using Contract Programming FrameworksLeverage existing contract programming tools like Python’s icontract, Java’s Contracts for Java, or Eiffel’s built-in contract support:\nimport icontract@icontract.require(lambda x: x &gt; 0, &quot;Input must be positive&quot;)@icontract.ensure(lambda result: result &gt; 0, &quot;Result must be positive&quot;)@icontract.snapshot(lambda x: x, &quot;Save original value&quot;)def calculate_square_root(x: float) -&gt; float:    # Implementation must satisfy pre- and post-conditions    return x ** 0.5\n\n3. Enhanced API DocumentationExplicitly list all implicit contracts in documentation using standardized templates:\n## Performance Characteristics- **Time complexity**: O(n)- **Space complexity**: O(1)- **Thread safety**: Yes## Side Effects- Modifies global configuration- Writes to log file## Error Handling- Raises `ValueError` for invalid input- Raises `RuntimeError` for insufficient resources\n\n4. Runtime Contract CheckingEnable contract checking in development and testing environments, disable in production for performance:\nclass ContractAwareProcessor:    def __init__(self, debug=False):        self.debug = debug        def process(self, data):        if self.debug:            self._check_preconditions(data)                result = self._actual_process(data)                if self.debug:            self._check_postconditions(data, result)                return result\n\n5. AI-Friendly Code AnnotationsProvide specialized annotations to help AI assistants understand implicit contracts:\n# @ai-contract: This function handles user input and is performance-sensitive# @ai-expectation: Should complete within 100ms# @ai-side-effect: Will update user status in database# @ai-error-case: Retry 3 times on network timeoutdef handle_user_request(request):    # Implementation...\n\nFuture-Oriented Thinking1. Contracts as First-Class CitizensFuture programming languages might treat contracts as first-class citizens, similar to type systems. Compilers could statically check contracts, and IDEs could provide better support.\n2. AI-Understandable Contract LanguagesWe need to develop contract description languages that are both human-friendly and AI-parsable, capable of expressing complex constraints and expectations.\n3. Contract Learning and InferenceAI systems could analyze large codebases to automatically learn and infer common implicit contracts, suggesting their explicit formalization.\n4. Contract-Driven Code GenerationFuture AI code generators could take contracts as input and generate implementations satisfying all constraints.\nPractical RecommendationsFor Developers:\nIdentify critical contracts: During code reviews, pay special attention to functions that may contain implicit contracts\nGradual explicitization: Don’t try to document all contracts at once; start with the most critical ones\nEstablish a contract culture: Promote contract-first thinking within your team\nLeverage tools: Use static analysis tools to detect potential contract violations\n\nFor AI Prompt Engineers:\nExplicitly express expectations: In prompts, specify not just “what to do” but also “under what constraints”\nProvide context: Tell AI about the environment where code will run and performance requirements\nRequire contract annotations: Ask AI-generated code to include explicit contract comments\nTest boundary conditions: Specifically test edge cases that might violate implicit contracts\n\nConclusion: The Evolution from Implicit to ExplicitThe problem of implicit contracts is an inevitable stage in the maturation of AI programming. Just as software engineering evolved from “writing code” to “designing systems,” from “it runs” to “it’s maintainable,” we’re now experiencing the evolution from “implicit understanding” to “explicit expression.”\nSolving implicit contract problems isn’t just about making AI better at programming—it’s about making all software more reliable, maintainable, and understandable. In this process, we’re not only teaching AI how to program but also teaching ourselves how to better express intent, manage complexity, and build robust systems.\nUltimately, explicit contracts will become crucial bridges connecting human intent, machine understanding, and code implementation. When these bridges are built, AI programming can truly evolve from “assistant tool” to “reliable partner.”\n\nFood for thought: What important implicit contracts exist in your projects? If AI were to maintain your code, which contracts would most need to be made explicit?\nFurther reading:\n\nDesign Patterns: Elements of Reusable Object-Oriented Software - Patterns themselves are a form of high-level contracts\nCode Complete - A comprehensive guide to software construction\nRefactoring: Improving the Design of Existing Code - How to safely modify code without violating contracts\n\n","categories":["English"],"tags":["English","AI Programming","Software Development","Code Quality"]},{"title":"AI's Open Source Productivity Explosion: How Do We Keep It Secure?","url":"/2026/03/02/ai-opensource-security-en/","content":"AI’s Open Source Productivity Explosion: How Do We Keep It Secure?Last year, a developer bragged on Twitter: used Copilot to write a complete backend system, finished in three days. Hundreds of replies below, half marveling at the efficiency, half asking “did you review the code?”\nHe never answered the second question.\nThe Productivity Explosion is RealThe data doesn’t lie. GitHub statistics show that AI-generated code commits increased 300% year-over-year in 2024. This isn’t incremental improvement—it’s an order-of-magnitude leap.\nI’ve noticed something strange in many open source projects lately: commit histories look different. Features that used to require days of iteration now often appear as single, massive commits. You can tell from the commit messages:\nfeat: add complete authentication system with JWT, refresh tokens, rate limiting, and email verification+2847 -0\n\nThat’s not something one person can write in three days, but AI can generate a first draft in three hours.\nHere’s the question: who’s reviewing those 2,847 lines?\nThe Code Review DilemmaTraditional code review assumes this model: developers write code, reviewers check logic, performance, and security line by line. This works when code volume is manageable.\nBut AI changed the game.\nTypical scenario: an intern uses ChatGPT to generate an authentication module. Looks feature-complete, passes tests. The reviewer spends half an hour browsing, finds no obvious issues, approves merge.\nThree months later, the security team discovers a timing attack vulnerability. Attackers can infer whether usernames exist by measuring response times.\nThis is the classic problem with AI-generated code: functionally correct, but containing non-obvious security flaws.\nWorse, reviewers face a subtle psychological trap when reviewing AI code: “This is AI-written, should be pretty standard, right?” This assumption is dangerous.\nNew Supply Chain ThreatsOpen source supply chain security was already hard. AI makes it harder.\nLast year, npm saw a batch of “seemingly normal” packages. Code structure was reasonable, documentation complete, even had unit tests. But closer inspection revealed these packages executed malicious code under specific conditions.\nSecurity researchers later confirmed these packages were likely AI-batch-generated. Attackers only needed to:\n\nUse AI to generate a seemingly useful package\nPlant malicious code at key points\nUse AI to generate “natural” commit history\nPublish to npm\n\nFrighteningly cheap, remarkably effective. Because these packages look indistinguishable from legitimate ones at first glance.\nWe’re no longer facing a few hackers handcrafting malicious packages. We might be facing industrialized, scaled supply chain attacks.\nWhy Traditional Solutions FailStatic analysis tools seem somewhat powerless against AI code.\nSimple reason: these tools rely on rules and pattern matching. But AI-generated code is often “too standard,” bypassing many static checks.\nFor example, traditional SQL injection detection flags code like:\nquery = &quot;SELECT * FROM users WHERE id = &quot; + user_id\n\nBut AI typically generates “safer-looking” code:\nquery = f&quot;SELECT * FROM users WHERE id = &#123;sanitize_input(user_id)&#125;&quot;\n\nProblem is: sanitize_input might not exist, or its implementation might be flawed. But seeing the “sanitization” step, static analysis tools might let it pass.\nManual review hit bottlenecks too. Facing thousands of lines of AI-generated code, reviewers struggle to maintain focus. Cognitive load is too high, easy to miss critical issues.\nWhat Kind of Security Solutions Do We NeedHonestly, I don’t have perfect answers. But from practice, a few directions seem promising.\n1. Intervene During GenerationRather than review after the fact, inject security constraints while AI generates code.\nPeople are already trying this approach. For example, explicitly requiring in prompts:\nGenerate user login endpoint, requirements:- Use parameterized queries to prevent SQL injection- Passwords must use bcrypt encryption, not MD5 or SHA1- Implement rate limiting, same IP max 5 attempts per 5 minutes- All error messages must be uniform, not reveal whether user exists\n\nThis generates much better code. But it requires developers themselves to have security awareness, knowing what to ask for.\n2. Build Secure Code LibrariesRather than letting AI generate from scratch each time, build a set of security-reviewed code templates.\nStripe’s approach is worth noting. They have an internal code snippet library. All code involving sensitive operations comes from this library. Developers can use AI assistance, but critical parts must use verified templates.\nNot a perfect solution, but at least ensures baseline security.\n3. Redesign Review ProcessesTraditional pull request review may no longer suit the AI era.\nSome teams are experimenting with new workflows:\n\nGrade AI-generated code: critical path code requires deep manual review\nIntroduce “security reviewer” role, specifically responsible for checking AI-generated code security\nUse differentiated review: AI-generated code and human-written code adopt different review standards\n\nThese experiments are ongoing, but the direction is right.\n4. Toolchain UpgradesWe need next-generation security tools specifically for AI-generated code characteristics.\nSome interesting attempts:\n\nSemantic analysis tools: not just syntax, but understanding code intent\nAnomaly pattern detection: flag code that “looks too perfect”\nAI vs AI: use AI to review AI-generated code\n\nThe last one sounds ironic, but might be most effective. After all, AI best understands what mistakes AI makes.\nWhere Are the Responsibility BoundariesThis is an even harder question.\nWhen AI-generated code has security issues, who’s responsible?\n\nDevelopers say: I just used a tool, how would I know the generated code has problems?\nAI providers say: Our terms of service say generated code needs manual review.\nCompanies say: We trust developers’ professional judgment.\n\nResult: nobody’s really responsible.\nThis responsibility ambiguity will have disastrous consequences. We need clear rules:\n\nDevelopers using AI to generate code have an obligation to understand and review generated code\nAI providers need to warn about known security issues\nOrganizations need clear AI code usage guidelines\n\nLaws and regulations may intervene, but before that, the industry needs self-regulation.\nSome Actionable SuggestionsFor individual developers:\n\nNever directly copy-paste AI-generated code, at least understand what it’s doing\nStay skeptical about security-related code, manually check critical logic\nLearn basic security knowledge, AI can’t replace your judgment\n\nFor teams:\n\nEstablish AI code usage guidelines, clarify which scenarios allow it, which don’t\nInvest in security training, ensure team understands AI code risks\nBuild layered review mechanisms, critical code must pass security expert review\n\nFor open source projects:\n\nState in README which parts used AI assistance\nConduct additional security review for AI-generated code\nEstablish vulnerability disclosure mechanisms, encourage security researchers to participate\n\nThis Isn’t AlarmismI’m not opposing AI-assisted development. Quite the opposite, I believe AI will become a standard development tool.\nBut we must face a fact: explosive productivity growth must be accompanied by synchronized security capability improvement.\nThe issue isn’t whether AI generates code with vulnerabilities—it definitely will. The issue is whether we’ve established sufficient mechanisms to identify and fix these vulnerabilities.\nCurrent situation: code generation speed increased 10x, but security review speed remains the same. This gap widens every day.\nIf we don’t close this gap soon, we might face an open source security crisis. Not because AI is malicious, but because our security mechanisms can’t keep up with productivity explosion.\nFinallyTechnological progress is always a double-edged sword. Steam engines brought industrial revolution, also brought environmental pollution. The internet connected the world, also created new crime spaces.\nAI dramatically improves development efficiency, while also amplifying security risks. This is the reality we must face.\nThe good news is, we still have time. The open source community has always been good at self-correction and evolution. As long as we recognize the problem’s severity and build security mechanisms adapted to the AI era, this productivity explosion will ultimately be positive.\nBut the time window won’t stay open forever. Now is the time to act.\n\nHow much code in your project is AI-generated? Have you reviewed it?\nThis isn’t questioning, it’s reminding. Including myself.\n","categories":["English"],"tags":["AI Security","Open Source Security","Code Security","Supply Chain Security"]},{"title":"AI导致开源生产率爆炸，安全如何保障？","url":"/2026/03/02/ai-opensource-security/","content":"AI导致开源生产率爆炸，安全如何保障？去年有个开发者在Twitter上炫耀：用Copilot写了一个完整的后台系统，三天完工。底下几百条回复，一半在惊叹效率，一半在问”你审查代码了吗”。\n他没有回答第二个问题。\n生产力爆炸正在发生数据不会骗人。GitHub的统计显示，2024年AI生成的代码提交量同比增长了300%。这不是渐进式的改进，而是量级的跃迁。\n我最近观察到一个现象：很多开源项目的提交历史变得很奇怪。以前一个功能可能需要几天的迭代，现在往往是一次性提交一大坨代码。看commit message能明显感觉出来：\nfeat: add complete authentication system with JWT, refresh tokens, rate limiting, and email verification+2847 -0\n\n这不是一个人三天能写完的东西，但AI可以在三个小时内生成初稿。\n问题来了：谁在审查这2847行代码？\n安全审查的困境传统的代码审查假设是这样的：开发者写代码，审查者逐行检查逻辑、性能、安全问题。这个模式在代码量可控的时候还能运转。\n但AI改变了游戏规则。\n一个典型场景：实习生用ChatGPT生成了一个用户认证模块，看起来功能完整，测试也通过了。代码审查者花了半小时浏览，没发现明显问题，批准合并。\n三个月后，安全团队发现这个模块存在时序攻击漏洞。攻击者可以通过测量响应时间推断用户名是否存在。\n这是AI生成代码的典型问题：功能正确，但存在非显而易见的安全隐患。\n更糟糕的是，审查者面对AI生成的代码时，会产生一种微妙的心理：这是AI写的，应该比较标准吧？这种假设是危险的。\n供应链的新威胁开源供应链安全本来就是个难题，AI让它变得更难了。\n去年npm仓库里出现了一批”看起来很正常”的包。它们的代码结构合理、文档完整、甚至有单元测试。但细查之下会发现，这些包在特定条件下会执行恶意代码。\n安全研究员后来确认，这些包很可能是用AI批量生成的。攻击者只需要：\n\n用AI生成一个看起来有用的包\n在关键位置植入恶意代码\n用AI生成”自然”的commit历史\n发布到npm\n\n成本低得可怕，效果却惊人。因为这些包从表面看和正常包没什么区别。\n我们现在面临的不再是几个黑客手工制作恶意包，而是可能面对工业化、规模化的供应链攻击。\n为什么传统方案失效了静态分析工具在AI代码面前显得有些无力。\n原因很简单：这些工具依赖规则和模式匹配。但AI生成的代码往往”太标准了”，反而绕过了很多静态检查。\n举个例子，传统的SQL注入检测会标记这样的代码：\nquery = &quot;SELECT * FROM users WHERE id = &quot; + user_id\n\n但AI通常会生成看起来”更安全”的代码：\nquery = f&quot;SELECT * FROM users WHERE id = &#123;sanitize_input(user_id)&#125;&quot;\n\n问题是：sanitize_input函数可能根本不存在，或者实现有漏洞。但静态分析工具看到有”清理”这个动作，就可能放过它。\n人工审查也遇到了瓶颈。面对成千上万行AI生成的代码，审查者很难保持专注。认知负荷太大，很容易漏掉关键问题。\n我们需要什么样的安全方案说实话，我没有完美答案。但从实践来看，有几个方向是靠谱的。\n1. 在生成时就介入与其事后审查，不如在AI生成代码时就加入安全约束。\n现在已经有人在尝试这个方向。比如在prompt里明确要求：\n请生成用户登录接口，要求：- 使用参数化查询防止SQL注入- 密码必须用bcrypt加密，不能用MD5或SHA1- 实现速率限制，同一IP 5分钟最多尝试5次- 所有错误信息必须统一，不能泄露用户是否存在\n\n这样生成的代码会好很多。但这要求开发者自己有安全意识，知道该提什么要求。\n2. 建立安全代码库与其让AI每次从零生成，不如建立一套经过审查的安全代码模板。\nStripe的做法值得参考。他们内部有一套代码片段库，所有涉及敏感操作的代码都来自这个库。开发者可以用AI辅助开发，但关键部分必须使用经过验证的模板。\n这不是完美方案，但至少能保证基础安全。\n3. 重新设计审查流程传统的pull request审查可能不再适用于AI时代。\n一些团队在尝试新的流程：\n\n对AI生成的代码进行分级：关键路径的代码必须人工深度审查\n引入”安全审查者”角色，专门负责检查AI生成代码的安全问题\n使用差异化审查：AI生成的代码和人写的代码采用不同的审查标准\n\n这些实验还在进行中，但方向是对的。\n4. 工具链的升级我们需要新一代的安全工具，专门针对AI生成代码的特点。\n有些有意思的尝试：\n\n语义分析工具：不只是看语法，还要理解代码的意图\n异常模式检测：标记那些”看起来太完美”的代码\nAI对抗AI：用AI来审查AI生成的代码\n\n最后一个听起来有点讽刺，但可能是最有效的。毕竟，AI最了解AI会犯什么错误。\n责任边界在哪里这是个更难的问题。\nAI生成的代码出了安全问题，谁负责？\n\n开发者说：我只是用了工具，工具生成的代码我怎么知道有问题？\nAI提供商说：我们的服务条款写了，生成的代码需要人工审查。\n公司说：我们信任开发者的专业判断。\n\n结果就是，没人真正负责。\n这种责任模糊会带来灾难性后果。我们需要建立明确的规则：\n\n使用AI生成代码的开发者，有义务理解和审查生成的代码\nAI提供商需要对已知的安全问题提供警告\n组织需要建立清晰的AI代码使用规范\n\n法律和监管可能会介入，但在那之前，行业需要自律。\n一些可行的建议对个人开发者：\n\n永远不要直接复制粘贴AI生成的代码，至少要理解它在做什么\n对涉及安全的代码保持怀疑，手动检查关键逻辑\n学习基本的安全知识，AI不能替代你的判断\n\n对团队：\n\n制定AI代码使用规范，明确哪些场景可以用，哪些不能用\n投资安全培训，确保团队理解AI代码的风险\n建立分层审查机制，关键代码必须经过安全专家审查\n\n对开源项目：\n\n在README里说明项目中哪些部分使用了AI辅助开发\n对AI生成的代码进行额外的安全审查\n建立漏洞披露机制，鼓励安全研究者参与\n\n这不是危言耸听我不是在反对AI辅助开发。恰恰相反，我认为AI会成为开发的标配工具。\n但我们必须正视一个事实：生产力的爆炸式增长，必须伴随着安全能力的同步提升。\n问题不在于AI会不会生成有漏洞的代码——它肯定会。问题在于，我们是否建立了足够的机制来识别和修复这些漏洞。\n现在的情况是：代码生成速度提升了10倍，但安全审查的速度还是原来那样。这个gap每天都在扩大。\n如果不尽快补上这个缺口，我们可能会迎来一场开源安全危机。不是因为AI恶意，而是因为我们的安全机制跟不上生产力的爆炸。\n最后技术进步总是双刃剑。蒸汽机带来了工业革命，也带来了环境污染。互联网连接了世界，也创造了新的犯罪空间。\nAI极大提升了开发效率，同时也放大了安全风险。这是我们必须面对的现实。\n好消息是，我们还有时间。开源社区一直善于自我纠错和进化。只要我们认识到问题的严重性，建立起适应AI时代的安全机制，这场生产力爆炸最终会是积极的。\n但时间窗口不会一直开着。现在就是行动的时候。\n\n你的项目里有多少代码是AI生成的？你审查过它们吗？\n这不是质疑，而是提醒。包括我自己。\n","categories":["技术思考"],"tags":["AI安全","开源安全","代码安全","供应链安全"]},{"title":"AI 会让社会发生怎样的变革","url":"/2026/03/01/ai-social-revolution/","content":"站在分岔路口每隔几十年，人类社会就会遇到一个分岔路口。\n蒸汽机来的时候，有人说它会解放人类的双手，也有人说它会让工人沦为机器的奴隶。两种预言都应验了——工业革命确实创造了前所未有的繁荣，也确实制造了血汗工厂和童工。\n互联网来的时候，有人说它会让信息自由流通、消除信息不对称，也有人说它会制造信息茧房和数字鸿沟。两种预言也都应验了。\n现在轮到 AI 了。\n关于 AI 会把社会带向何方，乐观者和悲观者的分歧之大，可能是历次技术革命中最极端的。乐观者说 AI 会带来人类文明的黄金时代，悲观者说 AI 可能是人类最后一个发明。\n两边都不是在危言耸听。这才是最让人不安的地方。\n今天我想认真地把两种观点都摊开来聊聊——不是为了站队，而是为了看清楚我们到底面对的是什么。\n乐观篇：AI 可能带来的美好医疗：每个人都能享受顶级医疗现在全球最好的医生有多少？几千个？几万个？不管多少，肯定不够 80 亿人用。\nAI 正在改变这个等式。AlphaFold 解决了蛋白质折叠问题，这意味着药物研发的速度可能提升一个数量级。AI 辅助诊断已经在某些领域（比如皮肤癌筛查、眼底病变检测）达到甚至超过了专科医生的水平。\n想象一下这样的未来：你身体不舒服，AI 根据你的基因组数据、生活习惯、历史病历，给出个性化的诊断和治疗方案。不需要排队三小时看病三分钟，不需要因为医疗资源不均衡而延误治疗。\n这不是科幻。很多技术已经在实验室里跑通了，缺的只是临床验证和监管审批。\n如果 AI 能让全球 80 亿人都享受到目前只有少数人能享受的医疗水平，这可能是人类历史上最大的福祉提升。\n教育：每个孩子都有私人导师教育的本质是什么？是一个有经验的人，根据学生的具体情况，用最适合他的方式传授知识。\n问题是，一个老师面对四五十个学生，不可能做到因材施教。这不是老师的错，是资源约束。\nAI 可以打破这个约束。一个 AI 导师可以：\n\n精确了解每个学生的知识盲点\n根据学生的学习风格调整教学方式\n无限耐心地解答问题\n24 小时随时可用\n覆盖从小学数学到量子物理的所有学科\n\n这意味着什么？意味着一个偏远山区的孩子，理论上可以获得和北京四中学生同等质量的教育资源。教育公平，可能第一次从口号变成现实。\n生产力：创造力的民主化我在前一篇文章里聊过，AI 正在让”一个人干翻一个团队”成为可能。这背后更深层的含义是：创造力正在被民主化。\n以前，把一个想法变成产品，需要资金、团队、技术能力。大多数人有想法但没有资源去实现。现在，AI 把实现想法的门槛降到了前所未有的低点。\n一个没学过编程的人，可以用 AI 做出一个 App。一个没学过画画的人，可以用 AI 生成精美的插画。一个没学过音乐的人，可以用 AI 创作一首歌。\n这不是说专业技能不重要了。而是说，更多的人有机会参与创造，而不是只能当消费者。 当创造者的数量从几百万变成几十亿，人类文明的创新速度会发生质的飞跃。\n科学：加速基础研究AI 在科学研究中的潜力可能是最被低估的。\n数学证明、材料发现、气候模拟、基因编辑……这些领域的突破往往需要处理海量数据和探索巨大的搜索空间。人类科学家的直觉很重要，但在某些维度上，AI 的计算能力是碾压性的。\nDeepMind 用 AI 发现了新的数学定理。AI 在材料科学中发现了人类从未想到过的新材料。这些不是替代科学家，而是给科学家装上了望远镜和显微镜——让他们看到以前看不到的东西。\n如果 AI 能帮助人类在可控核聚变、癌症治疗、气候变化等关键问题上取得突破，那它对人类文明的贡献将是不可估量的。\n\n悲观篇：AI 可能带来的风险就业：大规模失业不是危言耸听每次技术革命都会消灭一批工作岗位，同时创造一批新的。蒸汽机消灭了手工织布工，但创造了工厂工人。互联网消灭了很多中间商，但创造了程序员和运营。\n乐观者说 AI 也一样——旧工作消失，新工作出现，总量不变甚至增加。\n但这次可能真的不一样。\n以前的技术革命替代的是体力劳动或简单的重复性脑力劳动。AI 替代的是认知劳动——写作、分析、编程、设计、翻译、客服、法律咨询……这些是中产阶级的核心技能。\n而且替代的速度可能远快于新工作的创造速度。工业革命花了几十年才完成转型，期间有足够的时间让社会适应。AI 的渗透速度是以年甚至月计算的。\n更关键的是：新创造的工作，可能需要更高的技能门槛。 不是所有被替代的客服人员都能转型成 AI 训练师。不是所有被替代的初级程序员都能升级成 AI 系统架构师。\n如果大量人口在短时间内失去工作，而又没有足够的社会安全网来缓冲，社会动荡几乎是必然的。\n不平等：技术鸿沟加剧贫富差距AI 的好处不是均匀分配的。\n谁最先享受到 AI 的红利？是那些有资金购买 AI 工具、有教育背景理解 AI 能力、有基础设施支撑 AI 运行的人和国家。\n一个硅谷的工程师用 Cursor 写代码，效率提升 5 倍。一个发展中国家的程序员可能连稳定的网络都没有。\n一个大公司用 AI 优化供应链，利润翻倍。一个小作坊的老板可能连 AI 是什么都不知道。\n技术革命的历史告诉我们：新技术在短期内几乎总是加剧不平等。 只有当技术充分普及、成本降到足够低之后，红利才会扩散到更广泛的人群。但在那之前，先行者和落后者之间的差距会被急剧拉大。\n国家层面也是如此。掌握 AI 核心技术的国家（主要是美国和中国）和其他国家之间的差距，可能会比互联网时代更大。\n信息：真假难辨的世界深度伪造（Deepfake）已经不是新闻了。但随着 AI 生成能力的提升，问题会变得更加严重。\n当 AI 可以生成以假乱真的视频、音频、文字，我们怎么判断什么是真的？当任何人都可以用 AI 批量生产虚假信息，民主社会赖以运转的”共识”基础会不会崩塌？\n这不是技术问题，是社会问题。技术上你可以做 AI 检测来识别伪造内容，但这是一场永远追不上的军备竞赛——生成技术永远比检测技术跑得快。\n更深层的问题是：当人们不再相信自己看到的和听到的，社会信任的基础会被侵蚀。 而信任是一切社会合作的前提。\n权力集中：少数人控制 AI 基础设施训练一个前沿大模型需要几亿甚至几十亿美元。这意味着只有极少数公司有能力做这件事。\nOpenAI、Google、Anthropic、Meta……全球能训练顶级大模型的公司，一只手数得过来。这些公司掌握着 AI 时代最核心的基础设施。\n这种集中度比互联网时代更极端。互联网时代，虽然 Google 和 Facebook 很大，但任何人都可以建一个网站。AI 时代，你可以用别人的 API，但你无法训练自己的基础模型。\n当少数公司掌握了 AI 的”发电厂”，它们对社会的影响力会超过很多国家政府。 它们的价值观、偏见、商业利益，会通过 AI 渗透到社会的每一个角落。\n存在性风险：失控的可能这是最极端的悲观观点，但提出这个观点的不是科幻作家，而是 AI 领域最顶尖的研究者。\nHinton（深度学习之父）从 Google 离职，专门警告 AI 的风险。Yoshua Bengio（另一位图灵奖得主）签署了呼吁暂停 AI 研发的公开信。OpenAI 自己成立了”超级对齐”团队（虽然后来团队解散了，这本身就说明问题）。\n他们担心的不是当前的 AI，而是未来可能出现的超级智能。如果一个 AI 系统的智能远超人类，而我们又无法确保它的目标和人类的利益一致，后果可能是灾难性的。\n这个风险有多大？没人知道。可能是 1%，可能是 10%，可能是 0.01%。但当赌注是整个人类文明时，即使是很小的概率也值得认真对待。\n\n被忽视的中间地带大多数关于 AI 未来的讨论，要么极度乐观，要么极度悲观。但现实往往在中间。\n技术革命的历史告诉我们：好的和坏的通常同时发生。\n工业革命带来了繁荣，也带来了污染和剥削。互联网带来了信息自由，也带来了隐私侵犯和注意力经济。AI 大概率也是如此——它会同时带来巨大的好处和巨大的问题。\n关键不在于 AI 本身是好是坏，而在于：\n我们选择怎么用它。\n同样的 AI 技术，可以用来做个性化教育，也可以用来做个性化操控。可以用来加速药物研发，也可以用来设计生化武器。可以用来帮助残障人士，也可以用来大规模监控。\n技术是中性的，但使用技术的社会不是。 制度、法律、文化、价值观——这些”软基建”决定了技术最终走向何方。\n我的判断聊了这么多，说说我自己的看法。\n短期（3-5 年），我偏悲观。 就业冲击会比大多数人预期的来得更快更猛。社会的适应速度跟不上技术的变化速度。会有一段混乱期。\n中期（5-15 年），我谨慎乐观。 新的工作形态会逐渐成型，社会制度会开始适应。AI 在医疗、教育、科研等领域的正面影响会越来越明显。但不平等问题会是一个持续的挑战。\n长期（15 年以上），我不确定。 这取决于太多我们现在无法预测的变量——AI 的发展速度、国际合作的程度、社会制度的演化、是否出现黑天鹅事件。\n但有一点我比较确定：被动等待是最差的策略。\n不管你是乐观还是悲观，最理性的做法是：积极了解 AI，学会使用 AI，同时关注 AI 带来的社会问题，推动负责任的 AI 发展。\n写在最后我想起一个老故事。\n汽车刚发明的时候，英国通过了一个”红旗法案”——规定每辆汽车前面必须有一个人举着红旗步行开道，汽车的速度不能超过步行速度。\n这个法案的初衷是保护行人安全，出发点没错。但结果是英国的汽车工业被德国和美国远远甩在后面。\n过度恐惧和盲目乐观一样危险。\nAI 会让社会发生深刻的变革，这一点毫无疑问。变革中会有阵痛、有风险、有人受益、有人受损。但历史告诉我们，技术革命的大方向是不可逆的——你可以影响它的走向，但不能阻止它的到来。\n与其争论 AI 是天使还是魔鬼，不如想想：我们怎么做，才能让天使的一面多一点，魔鬼的一面少一点？\n这个问题的答案，不在 AI 手里，在我们手里。\n","categories":["技术"],"tags":["AI","技术思考","社会"]},{"title":"Will Rust Be the Best Programming Language for the AI Era?","url":"/2026/03/02/rust-ai-language-en/","content":"When AI Meets Rust: A Fateful Encounter?In recent years, the pace of AI development has been dizzying. From large language models to generative AI, from autonomous driving to robotics, AI is permeating every aspect of our lives. Meanwhile, a programming language called Rust has been quietly rising—it has topped Stack Overflow’s “Most Loved Programming Language” survey for eight consecutive years.\nThis raises an intriguing question: when the AI wave meets Rust’s rise, what kind of chemical reaction will occur? Could Rust become the best programming language for the AI era?\nRust’s Three Aces1. Memory Safety, No Garbage Collection NeededIn the AI field, memory management is a headache. Python is simple and easy to use, but the pauses caused by GC (garbage collection) can be fatal in real-time systems. C++ is powerful in performance, but memory safety issues keep developers up at night.\nRust’s uniqueness lies in: it guarantees memory safety at compile time. Through its ownership, borrowing, and lifetime systems, Rust lets you write code that is both safe and efficient, without runtime garbage collection.\nImagine a memory leak in an autonomous driving system that could lead to catastrophic consequences. Rust’s compile-time checks are like installing a safety door for AI systems.\n2. Fearless Concurrency, AI’s Natural PartnerAI applications are inherently concurrent. Model training requires distributed computing, inference services need to handle thousands of concurrent requests, and robotic systems must process perception, decision-making, and control across multiple threads simultaneously.\nRust’s concurrency model is another highlight. It prevents data races through its type system, allowing developers to practice “fearless concurrency.” This means you can confidently write multithreaded code without worrying about those hard-to-debug concurrency bugs.\n3. Performance Comparable to C&#x2F;C++, Ecosystem Growing RapidlyRust’s performance is comparable to C&#x2F;C++, and in some scenarios, it’s even better. This is crucial for compute-intensive AI applications. More importantly, Rust’s ecosystem is developing rapidly:\n\nML Frameworks: Rust ML frameworks like Burning, Candle, and Linfa are maturing\nWeb Frameworks: Actix, Rocket, and Axum provide high-performance backends for AI services\nEmbedded: Rust has clear advantages in embedded AI (edge computing)\nWASM Support: Rust is a first-class citizen for WebAssembly, suitable for browser-side AI\n\nRust’s Practical Applications in AICase 1: Hugging Face’s Tokenizers LibraryHugging Face is the GitHub of the AI world. Their tokenizers library was originally written in Python and later rewritten in Rust. The result? Performance improved by 10-100 times, with significantly reduced memory usage.\nCase 2: Microsoft’s Windows AI PlatformMicrosoft is introducing Rust into the Windows kernel and AI platform. They found that components rewritten in Rust are not only safer but also perform better. In critical paths like AI inference, Rust is becoming the preferred choice.\nCase 3: Autonomous Driving Company WayveThis UK-based autonomous driving startup uses Rust extensively. Their CTO stated: “Rust allows us to rapidly iterate complex perception and control systems while maintaining extremely high safety standards.”\nChallenges and ObstaclesOf course, Rust also faces challenges in the AI field:\n1. Steep Learning CurveRust’s ownership system and lifetime concepts take time to master. For AI researchers accustomed to Python, this barrier is not low.\n2. Ecosystem Still ImmatureAlthough Rust’s ML ecosystem is developing rapidly, there’s still a significant gap compared to Python’s PyTorch and TensorFlow. Many of the latest AI papers and models still provide Python implementations first.\n3. Community Culture DifferencesThe AI community is known for rapid experimentation and iteration, while the Rust community focuses more on correctness and safety. These two cultures need time to merge.\nFuture Outlook: Rust’s Role in the AI EraI believe Rust won’t completely replace Python’s position in AI research, but it will shine in the following areas:\n1. Production Deployment: Transforming research models into reliable production services2. Edge Computing: Running AI models on resource-constrained devices3. Infrastructure: Building AI training and inference infrastructure4. Safety-Critical Systems: Autonomous driving, medical AI, and other fields with extremely high safety requirementsAdvice for DevelopersIf you’re an AI developer, I recommend:\n\nDon’t Switch Completely: Continue using Python for research and prototyping, use Rust for production deployment\nStart with Infrastructure: First rewrite performance bottlenecks or safety-critical components in Rust\nFocus on Hybrid Architecture: Python handles upper-level logic, Rust handles underlying computation\nParticipate in Community Building: Rust’s AI ecosystem needs more developer contributions\n\nConclusionRust may not be the “only” programming language of the AI era, but it could well be one of the “best” programming languages—especially in scenarios with extremely high requirements for performance, safety, and reliability.\nJust as C defined systems programming, Java defined enterprise applications, and JavaScript defined web development, Rust has the opportunity to define the production-grade code standards for the AI era.\nThe future of AI needs not only clever algorithms but also reliable implementations. And Rust was born for reliability.\n\nFurther Reading:\n\nThe Rust Programming Language\nRust for AI&#x2F;ML\nHugging Face’s Rust tokenizers\n\nDiscussion: What do you think is Rust’s biggest opportunity in the AI field? Feel free to share your thoughts in the comments.\n","categories":["English"],"tags":["AI","English","Rust","Programming Languages","Technology Foresight"]},{"title":"Rust 会是 AI 时代的最好的编程语言吗？","url":"/2026/03/02/rust-ai-language/","content":"当 AI 遇上 Rust：一场命中注定的相遇？最近几年，AI 的发展速度让人眼花缭乱。从大语言模型到生成式 AI，从自动驾驶到机器人，AI 正在渗透到我们生活的方方面面。与此同时，一门名为 Rust 的编程语言也在悄然崛起——它连续八年蝉联 Stack Overflow “最受开发者喜爱语言”榜首。\n这不禁让人思考：当 AI 的浪潮遇上 Rust 的崛起，会产生怎样的化学反应？Rust 会成为 AI 时代的最佳编程语言吗？\nRust 的三大王牌1. 内存安全，无需垃圾回收在 AI 领域，内存管理是个头疼的问题。Python 虽然简单易用，但 GC（垃圾回收）带来的停顿在实时系统中可能是致命的。C++ 性能强大，但内存安全问题让开发者夜不能寐。\nRust 的独特之处在于：它在编译期就保证了内存安全。通过所有权（ownership）、借用（borrowing）和生命周期（lifetime）系统，Rust 让你写出既安全又高效的代码，而无需运行时垃圾回收。\n想象一下，在自动驾驶系统中，一个内存泄漏可能导致灾难性后果。Rust 的编译期检查，就像是为 AI 系统装上了一道安全门。\n2. 无畏并发，AI 的天然伙伴AI 应用天生就是并发的。模型训练需要分布式计算，推理服务要处理成千上万的并发请求，机器人系统要同时处理感知、决策、控制多个线程。\nRust 的并发模型是其另一大亮点。它通过类型系统防止数据竞争，让开发者可以”无畏并发”（fearless concurrency）。这意味着你可以放心地编写多线程代码，而不必担心那些难以调试的并发 bug。\n3. 性能媲美 C&#x2F;C++，生态日益完善Rust 的性能与 C&#x2F;C++ 相当，在某些场景下甚至更优。这对于计算密集型的 AI 应用至关重要。更重要的是，Rust 的生态系统正在快速发展：\n\nML 框架：Burning、Candle、Linfa 等 Rust ML 框架正在成熟\nWeb 框架：Actix、Rocket、Axum 为 AI 服务提供高性能后端\n嵌入式：Rust 在嵌入式 AI（边缘计算）领域优势明显\nWASM 支持：Rust 是 WebAssembly 的一等公民，适合浏览器端 AI\n\nRust 在 AI 领域的实际应用案例 1：Hugging Face 的 tokenizers 库Hugging Face 是 AI 界的 GitHub，他们的 tokenizers 库最初用 Python 编写，后来用 Rust 重写。结果？性能提升了 10-100 倍，同时内存使用大幅减少。\n案例 2：Microsoft 的 Windows AI 平台微软正在将 Rust 引入 Windows 内核和 AI 平台。他们发现，用 Rust 重写的组件不仅更安全，而且性能更好。在 AI 推理等关键路径上，Rust 正在成为首选。\n案例 3：自动驾驶公司 Wayve这家英国的自动驾驶初创公司大量使用 Rust。他们的 CTO 表示：”Rust 让我们能够快速迭代复杂的感知和控制系统，同时保持极高的安全标准。”\n挑战与障碍当然，Rust 在 AI 领域也面临挑战：\n1. 学习曲线陡峭Rust 的所有权系统和生命周期概念需要时间掌握。对于习惯了 Python 的 AI 研究者来说，这个门槛不低。\n2. 生态仍不完善虽然 Rust 的 ML 生态在快速发展，但与 Python 的 PyTorch、TensorFlow 相比，还有很大差距。许多最新的 AI 论文和模型仍然首先提供 Python 实现。\n3. 社区文化差异AI 社区以快速实验和迭代著称，而 Rust 社区更注重正确性和安全性。这两种文化需要时间融合。\n未来展望：Rust 在 AI 时代的角色我认为 Rust 不会完全取代 Python 在 AI 研究中的地位，但它会在以下几个领域大放异彩：\n1. 生产部署：将研究模型转化为可靠的生产服务2. 边缘计算：在资源受限的设备上运行 AI 模型3. 基础设施：构建 AI 训练和推理的基础设施4. 安全关键系统：自动驾驶、医疗 AI 等对安全性要求极高的领域给开发者的建议如果你是一名 AI 开发者，我建议：\n\n不要全盘切换：继续用 Python 做研究和原型，用 Rust 做生产部署\n从基础设施开始：先用 Rust 重写性能瓶颈或安全关键的组件\n关注混合架构：Python 负责上层逻辑，Rust 负责底层计算\n参与社区建设：Rust 的 AI 生态需要更多开发者贡献\n\n结语Rust 可能不是 AI 时代的”唯一”编程语言，但它很可能是”最好”的编程语言之一——特别是在对性能、安全和可靠性要求极高的场景中。\n就像 C 语言定义了系统编程，Java 定义了企业应用，JavaScript 定义了 Web 开发一样，Rust 有机会定义 AI 时代的生产级代码标准。\nAI 的未来不仅需要聪明的算法，更需要可靠的实现。而 Rust，正是为了可靠而生。\n\n延伸阅读：\n\nThe Rust Programming Language\nRust for AI&#x2F;ML\nHugging Face’s Rust tokenizers\n\n讨论：你认为 Rust 在 AI 领域的最大机会是什么？欢迎在评论区分享你的看法。\n","categories":["技术"],"tags":["AI","Rust","编程语言","技术前瞻"]},{"title":"How AI Will Transform Society","url":"/2026/03/01/ai-social-revolution-en/","content":"Standing at the CrossroadsEvery few decades, human society encounters a crossroads.\nWhen the steam engine arrived, some said it would liberate human hands. Others said it would reduce workers to slaves of machines. Both predictions came true — the Industrial Revolution created unprecedented prosperity and also produced sweatshops and child labor.\nWhen the internet arrived, some said it would enable free information flow and eliminate information asymmetry. Others said it would create filter bubbles and digital divides. Both predictions also came true.\nNow it’s AI’s turn.\nThe divide between optimists and pessimists about where AI will take society may be the most extreme in the history of technological revolutions. Optimists say AI will usher in a golden age of human civilization. Pessimists say AI might be humanity’s last invention.\nNeither side is being alarmist. That’s what makes it so unsettling.\nToday I want to lay both perspectives out honestly — not to pick sides, but to see clearly what we’re actually facing.\nThe Optimistic Case: The Beautiful PossibilitiesHealthcare: World-Class Medicine for EveryoneHow many of the world’s best doctors are there? A few thousand? Tens of thousands? Whatever the number, it’s certainly not enough for 8 billion people.\nAI is changing this equation. AlphaFold solved the protein folding problem, meaning drug development speed could improve by an order of magnitude. AI-assisted diagnosis has already matched or exceeded specialist physicians in certain areas (like skin cancer screening and retinal disease detection).\nImagine this future: you feel unwell, and AI generates a personalized diagnosis and treatment plan based on your genomic data, lifestyle habits, and medical history. No waiting three hours for a three-minute consultation. No delayed treatment due to uneven medical resource distribution.\nThis isn’t science fiction. Many of these technologies already work in labs — what’s missing is clinical validation and regulatory approval.\nIf AI can bring the medical standard currently available only to the few to all 8 billion people globally, this could be the greatest welfare improvement in human history.\nEducation: A Private Tutor for Every ChildWhat’s the essence of education? An experienced person teaching knowledge in the most suitable way based on each student’s specific situation.\nThe problem is, one teacher facing forty or fifty students simply can’t personalize instruction. That’s not the teacher’s fault — it’s a resource constraint.\nAI can break this constraint. An AI tutor can:\n\nPrecisely identify each student’s knowledge gaps\nAdjust teaching methods to match learning styles\nAnswer questions with infinite patience\nBe available 24&#x2F;7\nCover everything from elementary math to quantum physics\n\nWhat does this mean? A child in a remote village could theoretically access the same quality education as a student at an elite school. Educational equity might, for the first time, move from slogan to reality.\nProductivity: Democratization of CreativityAs I discussed in my previous article, AI is making it possible for “one person to outperform a team.” The deeper implication: creativity is being democratized.\nPreviously, turning an idea into a product required capital, a team, and technical skills. Most people had ideas but lacked resources to realize them. Now, AI has lowered the barrier to implementation to unprecedented levels.\nSomeone who never learned programming can build an app with AI. Someone who never learned to draw can generate beautiful illustrations. Someone who never studied music can compose a song.\nThis doesn’t mean professional skills don’t matter anymore. It means more people get to participate in creation, rather than being limited to consumption. When the number of creators goes from millions to billions, the pace of human innovation will undergo a qualitative leap.\nScience: Accelerating Fundamental ResearchAI’s potential in scientific research may be the most underestimated.\nMathematical proofs, materials discovery, climate simulation, gene editing… breakthroughs in these fields often require processing massive data and exploring enormous search spaces. Human scientists’ intuition matters, but in certain dimensions, AI’s computational power is overwhelming.\nDeepMind used AI to discover new mathematical theorems. AI has found materials in materials science that humans never conceived of. These aren’t replacing scientists — they’re equipping scientists with telescopes and microscopes, letting them see what was previously invisible.\nIf AI can help humanity achieve breakthroughs in controlled nuclear fusion, cancer treatment, and climate change, its contribution to human civilization will be immeasurable.\n\nThe Pessimistic Case: The RisksEmployment: Mass Unemployment Isn’t AlarmistEvery technological revolution eliminates some jobs while creating new ones. The steam engine eliminated hand weavers but created factory workers. The internet eliminated many middlemen but created programmers and content creators.\nOptimists say AI will be the same — old jobs disappear, new ones emerge, total employment stays the same or even grows.\nBut this time might truly be different.\nPrevious technological revolutions replaced physical labor or simple repetitive cognitive work. AI replaces cognitive labor — writing, analysis, programming, design, translation, customer service, legal consulting… These are the core skills of the middle class.\nAnd the replacement speed may far exceed the creation speed of new jobs. The Industrial Revolution took decades to complete its transition, giving society enough time to adapt. AI’s penetration speed is measured in years, even months.\nMore critically: newly created jobs may require higher skill thresholds. Not every displaced customer service rep can retrain as an AI trainer. Not every displaced junior programmer can upgrade to an AI systems architect.\nIf large populations lose their jobs in a short time without adequate social safety nets, social upheaval is almost inevitable.\nInequality: The Tech Divide Deepens Wealth GapsAI’s benefits aren’t evenly distributed.\nWho benefits from AI first? Those with capital to purchase AI tools, education to understand AI capabilities, and infrastructure to support AI operations — both people and nations.\nA Silicon Valley engineer uses Cursor to code, boosting productivity 5x. A programmer in a developing country might not even have stable internet.\nA large corporation uses AI to optimize its supply chain, doubling profits. A small workshop owner might not even know what AI is.\nThe history of technological revolutions tells us: new technology almost always exacerbates inequality in the short term. Only after technology becomes sufficiently widespread and costs drop low enough do benefits spread to broader populations. But before that, the gap between early adopters and laggards widens dramatically.\nThe same applies at the national level. The gap between countries mastering core AI technology (primarily the US and China) and others could be larger than during the internet era.\nInformation: A World Where Truth Is IndistinguishableDeepfakes are old news. But as AI generation capabilities improve, the problem will become far more severe.\nWhen AI can generate indistinguishable fake videos, audio, and text, how do we determine what’s real? When anyone can mass-produce disinformation with AI, will the “consensus” foundation that democratic societies depend on collapse?\nThis isn’t a technical problem — it’s a social one. Technically you can build AI detection to identify fakes, but it’s an arms race you can never win — generation technology always outpaces detection.\nThe deeper issue: when people no longer trust what they see and hear, the foundation of social trust erodes. And trust is the prerequisite for all social cooperation.\nPower Concentration: A Few Control AI InfrastructureTraining a frontier large model costs hundreds of millions to billions of dollars. This means only a handful of companies can do it.\nOpenAI, Google, Anthropic, Meta… the companies capable of training top-tier models can be counted on one hand. These companies control the most critical infrastructure of the AI era.\nThis concentration is more extreme than the internet era. During the internet era, though Google and Facebook were huge, anyone could build a website. In the AI era, you can use others’ APIs, but you can’t train your own foundation model.\nWhen a few companies control AI’s “power plants,” their influence on society will exceed many national governments. Their values, biases, and commercial interests will permeate every corner of society through AI.\nExistential Risk: The Possibility of Loss of ControlThis is the most extreme pessimistic view, but it’s not proposed by science fiction writers — it comes from AI’s most elite researchers.\nHinton (the godfather of deep learning) left Google specifically to warn about AI risks. Yoshua Bengio (another Turing Award winner) signed an open letter calling for a pause in AI development. OpenAI itself established a “superalignment” team (though the team later disbanded, which itself is telling).\nThey’re not worried about current AI, but about potential future superintelligence. If an AI system’s intelligence far exceeds humanity’s, and we can’t ensure its goals align with human interests, the consequences could be catastrophic.\nHow large is this risk? Nobody knows. Maybe 1%, maybe 10%, maybe 0.01%. But when the stakes are all of human civilization, even a small probability deserves serious attention.\n\nThe Overlooked Middle GroundMost discussions about AI’s future are either extremely optimistic or extremely pessimistic. But reality usually falls in between.\nThe history of technological revolutions tells us: good and bad typically happen simultaneously.\nThe Industrial Revolution brought prosperity and also pollution and exploitation. The internet brought information freedom and also privacy invasion and the attention economy. AI will most likely be the same — simultaneously bringing enormous benefits and enormous problems.\nThe key isn’t whether AI itself is good or bad, but:\nHow we choose to use it.\nThe same AI technology can be used for personalized education or personalized manipulation. To accelerate drug development or to design bioweapons. To help people with disabilities or for mass surveillance.\nTechnology is neutral, but the society using technology is not. Institutions, laws, culture, values — this “soft infrastructure” determines where technology ultimately leads.\nMy AssessmentAfter discussing all this, here’s my personal take.\nShort-term (3-5 years), I lean pessimistic. Employment shocks will come faster and harder than most people expect. Society’s adaptation speed can’t keep up with technology’s pace of change. There will be a period of turbulence.\nMedium-term (5-15 years), I’m cautiously optimistic. New work paradigms will gradually take shape, social systems will begin adapting. AI’s positive impact in healthcare, education, and research will become increasingly apparent. But inequality will remain a persistent challenge.\nLong-term (15+ years), I’m uncertain. This depends on too many variables we can’t currently predict — AI’s development speed, degree of international cooperation, evolution of social systems, whether black swan events occur.\nBut one thing I’m fairly certain about: passive waiting is the worst strategy.\nWhether you’re optimistic or pessimistic, the most rational approach is: actively understand AI, learn to use AI, while also paying attention to the social problems AI creates and pushing for responsible AI development.\nFinal ThoughtsI’m reminded of an old story.\nWhen cars were first invented, Britain passed the “Red Flag Act” — requiring every car to be preceded by a person walking with a red flag, with the car’s speed not exceeding walking pace.\nThe law’s intention was to protect pedestrian safety, and the motivation was sound. But the result was that Britain’s automotive industry fell far behind Germany and America.\nExcessive fear is as dangerous as blind optimism.\nAI will profoundly transform society — there’s no doubt about that. The transformation will involve growing pains, risks, winners, and losers. But history tells us that the general direction of technological revolution is irreversible — you can influence its trajectory, but you can’t stop its arrival.\nRather than debating whether AI is angel or demon, perhaps we should ask: what can we do to amplify the angel and diminish the demon?\nThe answer to that question isn’t in AI’s hands. It’s in ours.\n","categories":["English"],"tags":["AI","Thoughts","English","Society"]},{"title":"Why Blog?","url":"/2026/02/28/why-blog-en/","content":"Ideas Fade AwayHave you ever had this experience: you suddenly figured out a technical problem while taking a shower, or a brilliant product idea popped into your head during a walk, but by the time you sat down at your computer, it had already become as vague as a dream?\nI’ve had it too many times.\nThe brain is for generating ideas, not for storing them. If you don’t write them down, those flashes of inspiration will disappear forever into the river of time.\nA blog is my external memory.\nWriting Is the Best Way to LearnFeynman once said that if you can’t explain a concept in simple language, you don’t truly understand it.\nWriting a technical blog is the practice of the Feynman Technique. When you try to write a concept clearly, you’ll discover gaps in what you thought you understood. Filling those gaps is the process of truly learning.\nWriting is not the result of learning — writing itself is learning.\nLeave a TraceThere’s more and more content on the internet, but less and less genuine personal expression. Content on social media is fleeting; algorithms decide what gets seen and what gets forgotten.\nA blog is your own space. No need to cater to algorithms, no need to chase traffic — just record honestly.\nTen years from now, looking back, these words will tell you how you used to think and the path you’ve walked.\n\nThis blog will cover:\n\nTech Notes — Frontend, toolchains, architecture\nProduct Thinking — UX and product design observations\nTools &amp; Productivity — Methods to work more efficiently\nThoughts — Fragments of life\n\nNot pursuing perfection, just pursuing authenticity.\nLet’s start writing.\n— Hongqi, February 2026\n","categories":["English"],"tags":["Thoughts","Writing","English"]},{"title":"为什么要写博客","url":"/2026/02/28/why-blog/","content":"想法会消失你有没有过这样的经历：洗澡的时候突然想通了一个技术问题，或者散步时冒出了一个绝妙的产品 idea，但等你坐到电脑前，它已经模糊得像一场梦。\n我有过太多次了。\n大脑是用来产生想法的，不是用来存储想法的。如果不写下来，那些灵光一闪的瞬间就会永远消失在时间里。\n博客就是我的外部记忆。\n写作是最好的学习费曼说过，如果你不能用简单的语言解释一个概念，说明你还没有真正理解它。\n写技术博客的过程就是费曼学习法的实践。当你试图把一个知识点写清楚的时候，你会发现自己以为懂了的地方其实还有漏洞。填补这些漏洞的过程，就是真正学会的过程。\n写作不是学习的结果，写作本身就是学习。\n留下痕迹互联网上的内容越来越多，但真正有价值的个人表达越来越少。社交媒体上的内容转瞬即逝，算法决定了什么被看到、什么被遗忘。\n博客是属于自己的一片空间。不需要迎合算法，不需要追求流量，只需要诚实地记录。\n十年后回头看，这些文字会告诉你，你曾经是怎样思考的，你走过了怎样的路。\n\n这个博客会记录：\n\n技术笔记 — 前端、工具链、架构设计\n产品思考 — 用户体验、产品设计\n工具效率 — 让工作更高效的方法\n随想 — 生活中的碎片想法\n\n不追求完美，只追求真实。\n开始写吧。\n— 红齐，2026年2月\n","categories":["随想"],"tags":["随想","写作"]}]